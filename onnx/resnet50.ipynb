{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd98080",
   "metadata": {},
   "source": [
    "### Export ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd98df",
   "metadata": {},
   "source": [
    "ONNX tạo export đồ thị độc lập với phần cứng, tức là dù export trên CPU hay GPU thì graph được tạo ra cũng không đổi. ONNXRuntime mới tối ưu phụ thuộc vào phần cứng, đây là nơi tạo ra sự khác biệt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e5cb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "model = resnet50(pretrained=True)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8830851",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONNX_PATH = \"resnet50.onnx\"\n",
    "IMG_SIZE = 224\n",
    "OPSET = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d57777b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0212 14:19:06.354000 947721 site-packages/torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
      "W0212 14:19:06.356000 947721 site-packages/torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
      "W0212 14:19:06.357000 947721 site-packages/torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n",
      "W0212 14:19:06.358000 947721 site-packages/torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `ResNet([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `ResNet([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmits/miniconda3/envs/dl/lib/python3.10/copyreg.py:101: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "  return cls.__new__(cls, *args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 106 of general pattern rewrite rules.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ONNXProgram(\n",
       "    model=\n",
       "        <\n",
       "            ir_version=10,\n",
       "            opset_imports={'': 18},\n",
       "            producer_name='pytorch',\n",
       "            producer_version='2.10.0+cu128',\n",
       "            domain=None,\n",
       "            model_version=None,\n",
       "        >\n",
       "        graph(\n",
       "            name=main_graph,\n",
       "            inputs=(\n",
       "                %\"input\"<FLOAT,[1,3,224,224]>\n",
       "            ),\n",
       "            outputs=(\n",
       "                %\"output\"<FLOAT,[1,1000]>\n",
       "            ),\n",
       "            initializers=(\n",
       "                %\"conv1.weight\"<FLOAT,[64,3,7,7]>{Tensor(...)},\n",
       "                %\"layer1.0.conv1.weight\"<FLOAT,[64,64,1,1]>{Tensor(...)},\n",
       "                %\"layer1.0.conv2.weight\"<FLOAT,[64,64,3,3]>{Tensor(...)},\n",
       "                %\"layer1.0.conv3.weight\"<FLOAT,[256,64,1,1]>{Tensor(...)},\n",
       "                %\"layer1.0.downsample.0.weight\"<FLOAT,[256,64,1,1]>{Tensor(...)},\n",
       "                %\"layer1.1.conv1.weight\"<FLOAT,[64,256,1,1]>{Tensor(...)},\n",
       "                %\"layer1.1.conv2.weight\"<FLOAT,[64,64,3,3]>{Tensor(...)},\n",
       "                %\"layer1.1.conv3.weight\"<FLOAT,[256,64,1,1]>{Tensor(...)},\n",
       "                %\"layer1.2.conv1.weight\"<FLOAT,[64,256,1,1]>{Tensor(...)},\n",
       "                %\"layer1.2.conv2.weight\"<FLOAT,[64,64,3,3]>{Tensor(...)},\n",
       "                %\"layer1.2.conv3.weight\"<FLOAT,[256,64,1,1]>{Tensor(...)},\n",
       "                %\"layer2.0.conv1.weight\"<FLOAT,[128,256,1,1]>{Tensor(...)},\n",
       "                %\"layer2.0.conv2.weight\"<FLOAT,[128,128,3,3]>{Tensor(...)},\n",
       "                %\"layer2.0.conv3.weight\"<FLOAT,[512,128,1,1]>{Tensor(...)},\n",
       "                %\"layer2.0.downsample.0.weight\"<FLOAT,[512,256,1,1]>{Tensor(...)},\n",
       "                %\"layer2.1.conv1.weight\"<FLOAT,[128,512,1,1]>{Tensor(...)},\n",
       "                %\"layer2.1.conv2.weight\"<FLOAT,[128,128,3,3]>{Tensor(...)},\n",
       "                %\"layer2.1.conv3.weight\"<FLOAT,[512,128,1,1]>{Tensor(...)},\n",
       "                %\"layer2.2.conv1.weight\"<FLOAT,[128,512,1,1]>{Tensor(...)},\n",
       "                %\"layer2.2.conv2.weight\"<FLOAT,[128,128,3,3]>{Tensor(...)},\n",
       "                %\"layer2.2.conv3.weight\"<FLOAT,[512,128,1,1]>{Tensor(...)},\n",
       "                %\"layer2.3.conv1.weight\"<FLOAT,[128,512,1,1]>{Tensor(...)},\n",
       "                %\"layer2.3.conv2.weight\"<FLOAT,[128,128,3,3]>{Tensor(...)},\n",
       "                %\"layer2.3.conv3.weight\"<FLOAT,[512,128,1,1]>{Tensor(...)},\n",
       "                %\"layer3.0.conv1.weight\"<FLOAT,[256,512,1,1]>{Tensor(...)},\n",
       "                %\"layer3.0.conv2.weight\"<FLOAT,[256,256,3,3]>{Tensor(...)},\n",
       "                %\"layer3.0.conv3.weight\"<FLOAT,[1024,256,1,1]>{Tensor(...)},\n",
       "                %\"layer3.0.downsample.0.weight\"<FLOAT,[1024,512,1,1]>{Tensor(...)},\n",
       "                %\"layer3.1.conv1.weight\"<FLOAT,[256,1024,1,1]>{Tensor(...)},\n",
       "                %\"layer3.1.conv2.weight\"<FLOAT,[256,256,3,3]>{Tensor(...)},\n",
       "                %\"layer3.1.conv3.weight\"<FLOAT,[1024,256,1,1]>{Tensor(...)},\n",
       "                %\"layer3.2.conv1.weight\"<FLOAT,[256,1024,1,1]>{Tensor(...)},\n",
       "                %\"layer3.2.conv2.weight\"<FLOAT,[256,256,3,3]>{Tensor(...)},\n",
       "                %\"layer3.2.conv3.weight\"<FLOAT,[1024,256,1,1]>{Tensor(...)},\n",
       "                %\"layer3.3.conv1.weight\"<FLOAT,[256,1024,1,1]>{Tensor(...)},\n",
       "                %\"layer3.3.conv2.weight\"<FLOAT,[256,256,3,3]>{Tensor(...)},\n",
       "                %\"layer3.3.conv3.weight\"<FLOAT,[1024,256,1,1]>{Tensor(...)},\n",
       "                %\"layer3.4.conv1.weight\"<FLOAT,[256,1024,1,1]>{Tensor(...)},\n",
       "                %\"layer3.4.conv2.weight\"<FLOAT,[256,256,3,3]>{Tensor(...)},\n",
       "                %\"layer3.4.conv3.weight\"<FLOAT,[1024,256,1,1]>{Tensor(...)},\n",
       "                %\"layer3.5.conv1.weight\"<FLOAT,[256,1024,1,1]>{Tensor(...)},\n",
       "                %\"layer3.5.conv2.weight\"<FLOAT,[256,256,3,3]>{Tensor(...)},\n",
       "                %\"layer3.5.conv3.weight\"<FLOAT,[1024,256,1,1]>{Tensor(...)},\n",
       "                %\"layer4.0.conv1.weight\"<FLOAT,[512,1024,1,1]>{Tensor(...)},\n",
       "                %\"layer4.0.conv2.weight\"<FLOAT,[512,512,3,3]>{Tensor(...)},\n",
       "                %\"layer4.0.conv3.weight\"<FLOAT,[2048,512,1,1]>{Tensor(...)},\n",
       "                %\"layer4.0.downsample.0.weight\"<FLOAT,[2048,1024,1,1]>{Tensor(...)},\n",
       "                %\"layer4.1.conv1.weight\"<FLOAT,[512,2048,1,1]>{Tensor(...)},\n",
       "                %\"layer4.1.conv2.weight\"<FLOAT,[512,512,3,3]>{Tensor(...)},\n",
       "                %\"layer4.1.conv3.weight\"<FLOAT,[2048,512,1,1]>{Tensor(...)},\n",
       "                %\"layer4.2.conv1.weight\"<FLOAT,[512,2048,1,1]>{Tensor(...)},\n",
       "                %\"layer4.2.conv2.weight\"<FLOAT,[512,512,3,3]>{Tensor(...)},\n",
       "                %\"layer4.2.conv3.weight\"<FLOAT,[2048,512,1,1]>{Tensor(...)},\n",
       "                %\"fc.weight\"<FLOAT,[1000,2048]>{TorchTensor(...)},\n",
       "                %\"fc.bias\"<FLOAT,[1000]>{TorchTensor(...)},\n",
       "                %\"val_589\"<INT64,[2]>{Tensor<INT64,[2]>(array([-1, -2]), name='val_589')},\n",
       "                %\"val_593\"<INT64,[2]>{Tensor<INT64,[2]>(array([   1, 2048]), name='val_593')},\n",
       "                %\"conv1.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"layer1.0.conv1.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"layer1.0.conv2.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"layer1.0.conv3.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer1.0.downsample.0.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer1.1.conv1.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"layer1.1.conv2.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"layer1.1.conv3.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer1.2.conv1.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"layer1.2.conv2.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"layer1.2.conv3.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer2.0.conv1.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer2.0.conv2.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer2.0.conv3.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
       "                %\"layer2.0.downsample.0.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
       "                %\"layer2.1.conv1.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer2.1.conv2.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer2.1.conv3.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
       "                %\"layer2.2.conv1.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer2.2.conv2.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer2.2.conv3.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
       "                %\"layer2.3.conv1.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer2.3.conv2.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer2.3.conv3.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
       "                %\"layer3.0.conv1.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer3.0.conv2.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer3.0.conv3.weight_bias\"<FLOAT,[1024]>{Tensor(...)},\n",
       "                %\"layer3.0.downsample.0.weight_bias\"<FLOAT,[1024]>{Tensor(...)},\n",
       "                %\"layer3.1.conv1.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer3.1.conv2.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer3.1.conv3.weight_bias\"<FLOAT,[1024]>{Tensor(...)},\n",
       "                %\"layer3.2.conv1.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer3.2.conv2.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer3.2.conv3.weight_bias\"<FLOAT,[1024]>{Tensor(...)},\n",
       "                %\"layer3.3.conv1.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer3.3.conv2.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer3.3.conv3.weight_bias\"<FLOAT,[1024]>{Tensor(...)},\n",
       "                %\"layer3.4.conv1.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer3.4.conv2.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer3.4.conv3.weight_bias\"<FLOAT,[1024]>{Tensor(...)},\n",
       "                %\"layer3.5.conv1.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer3.5.conv2.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer3.5.conv3.weight_bias\"<FLOAT,[1024]>{Tensor(...)},\n",
       "                %\"layer4.0.conv1.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
       "                %\"layer4.0.conv2.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
       "                %\"layer4.0.conv3.weight_bias\"<FLOAT,[2048]>{Tensor(...)},\n",
       "                %\"layer4.0.downsample.0.weight_bias\"<FLOAT,[2048]>{Tensor(...)},\n",
       "                %\"layer4.1.conv1.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
       "                %\"layer4.1.conv2.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
       "                %\"layer4.1.conv3.weight_bias\"<FLOAT,[2048]>{Tensor(...)},\n",
       "                %\"layer4.2.conv1.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
       "                %\"layer4.2.conv2.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
       "                %\"layer4.2.conv3.weight_bias\"<FLOAT,[2048]>{Tensor(...)}\n",
       "            ),\n",
       "        ) {\n",
       "              0 |  # node_Conv_753\n",
       "                   %\"getitem\"<FLOAT,[1,64,112,112]> ⬅️ ::Conv(%\"input\", %\"conv1.weight\"{...}, %\"conv1.weight_bias\"{...}) {group=1, pads=(3, 3, 3, 3), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "              1 |  # node_relu\n",
       "                   %\"relu\"<FLOAT,[1,64,112,112]> ⬅️ ::Relu(%\"getitem\")\n",
       "              2 |  # node_max_pool2d\n",
       "                   %\"max_pool2d\"<FLOAT,[1,64,56,56]> ⬅️ ::MaxPool(%\"relu\") {storage_order=0, dilations=(1, 1), ceil_mode=0, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(2, 2), kernel_shape=(3, 3)}\n",
       "              3 |  # node_Conv_755\n",
       "                   %\"getitem_3\"<FLOAT,[1,64,56,56]> ⬅️ ::Conv(%\"max_pool2d\", %\"layer1.0.conv1.weight\"{...}, %\"layer1.0.conv1.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "              4 |  # node_relu_1\n",
       "                   %\"relu_1\"<FLOAT,[1,64,56,56]> ⬅️ ::Relu(%\"getitem_3\")\n",
       "              5 |  # node_Conv_757\n",
       "                   %\"getitem_6\"<FLOAT,[1,64,56,56]> ⬅️ ::Conv(%\"relu_1\", %\"layer1.0.conv2.weight\"{...}, %\"layer1.0.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "              6 |  # node_relu_2\n",
       "                   %\"relu_2\"<FLOAT,[1,64,56,56]> ⬅️ ::Relu(%\"getitem_6\")\n",
       "              7 |  # node_Conv_759\n",
       "                   %\"getitem_9\"<FLOAT,[1,256,56,56]> ⬅️ ::Conv(%\"relu_2\", %\"layer1.0.conv3.weight\"{...}, %\"layer1.0.conv3.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "              8 |  # node_Conv_761\n",
       "                   %\"getitem_12\"<FLOAT,[1,256,56,56]> ⬅️ ::Conv(%\"max_pool2d\", %\"layer1.0.downsample.0.weight\"{...}, %\"layer1.0.downsample.0.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "              9 |  # node_add\n",
       "                   %\"add\"<FLOAT,[1,256,56,56]> ⬅️ ::Add(%\"getitem_9\", %\"getitem_12\")\n",
       "             10 |  # node_relu_3\n",
       "                   %\"relu_3\"<FLOAT,[1,256,56,56]> ⬅️ ::Relu(%\"add\")\n",
       "             11 |  # node_Conv_763\n",
       "                   %\"getitem_15\"<FLOAT,[1,64,56,56]> ⬅️ ::Conv(%\"relu_3\", %\"layer1.1.conv1.weight\"{...}, %\"layer1.1.conv1.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             12 |  # node_relu_4\n",
       "                   %\"relu_4\"<FLOAT,[1,64,56,56]> ⬅️ ::Relu(%\"getitem_15\")\n",
       "             13 |  # node_Conv_765\n",
       "                   %\"getitem_18\"<FLOAT,[1,64,56,56]> ⬅️ ::Conv(%\"relu_4\", %\"layer1.1.conv2.weight\"{...}, %\"layer1.1.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             14 |  # node_relu_5\n",
       "                   %\"relu_5\"<FLOAT,[1,64,56,56]> ⬅️ ::Relu(%\"getitem_18\")\n",
       "             15 |  # node_Conv_767\n",
       "                   %\"getitem_21\"<FLOAT,[1,256,56,56]> ⬅️ ::Conv(%\"relu_5\", %\"layer1.1.conv3.weight\"{...}, %\"layer1.1.conv3.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             16 |  # node_add_1\n",
       "                   %\"add_1\"<FLOAT,[1,256,56,56]> ⬅️ ::Add(%\"getitem_21\", %\"relu_3\")\n",
       "             17 |  # node_relu_6\n",
       "                   %\"relu_6\"<FLOAT,[1,256,56,56]> ⬅️ ::Relu(%\"add_1\")\n",
       "             18 |  # node_Conv_769\n",
       "                   %\"getitem_24\"<FLOAT,[1,64,56,56]> ⬅️ ::Conv(%\"relu_6\", %\"layer1.2.conv1.weight\"{...}, %\"layer1.2.conv1.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             19 |  # node_relu_7\n",
       "                   %\"relu_7\"<FLOAT,[1,64,56,56]> ⬅️ ::Relu(%\"getitem_24\")\n",
       "             20 |  # node_Conv_771\n",
       "                   %\"getitem_27\"<FLOAT,[1,64,56,56]> ⬅️ ::Conv(%\"relu_7\", %\"layer1.2.conv2.weight\"{...}, %\"layer1.2.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             21 |  # node_relu_8\n",
       "                   %\"relu_8\"<FLOAT,[1,64,56,56]> ⬅️ ::Relu(%\"getitem_27\")\n",
       "             22 |  # node_Conv_773\n",
       "                   %\"getitem_30\"<FLOAT,[1,256,56,56]> ⬅️ ::Conv(%\"relu_8\", %\"layer1.2.conv3.weight\"{...}, %\"layer1.2.conv3.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             23 |  # node_add_2\n",
       "                   %\"add_2\"<FLOAT,[1,256,56,56]> ⬅️ ::Add(%\"getitem_30\", %\"relu_6\")\n",
       "             24 |  # node_relu_9\n",
       "                   %\"relu_9\"<FLOAT,[1,256,56,56]> ⬅️ ::Relu(%\"add_2\")\n",
       "             25 |  # node_Conv_775\n",
       "                   %\"getitem_33\"<FLOAT,[1,128,56,56]> ⬅️ ::Conv(%\"relu_9\", %\"layer2.0.conv1.weight\"{...}, %\"layer2.0.conv1.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             26 |  # node_relu_10\n",
       "                   %\"relu_10\"<FLOAT,[1,128,56,56]> ⬅️ ::Relu(%\"getitem_33\")\n",
       "             27 |  # node_Conv_777\n",
       "                   %\"getitem_36\"<FLOAT,[1,128,28,28]> ⬅️ ::Conv(%\"relu_10\", %\"layer2.0.conv2.weight\"{...}, %\"layer2.0.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "             28 |  # node_relu_11\n",
       "                   %\"relu_11\"<FLOAT,[1,128,28,28]> ⬅️ ::Relu(%\"getitem_36\")\n",
       "             29 |  # node_Conv_779\n",
       "                   %\"getitem_39\"<FLOAT,[1,512,28,28]> ⬅️ ::Conv(%\"relu_11\", %\"layer2.0.conv3.weight\"{...}, %\"layer2.0.conv3.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             30 |  # node_Conv_781\n",
       "                   %\"getitem_42\"<FLOAT,[1,512,28,28]> ⬅️ ::Conv(%\"relu_9\", %\"layer2.0.downsample.0.weight\"{...}, %\"layer2.0.downsample.0.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "             31 |  # node_add_3\n",
       "                   %\"add_3\"<FLOAT,[1,512,28,28]> ⬅️ ::Add(%\"getitem_39\", %\"getitem_42\")\n",
       "             32 |  # node_relu_12\n",
       "                   %\"relu_12\"<FLOAT,[1,512,28,28]> ⬅️ ::Relu(%\"add_3\")\n",
       "             33 |  # node_Conv_783\n",
       "                   %\"getitem_45\"<FLOAT,[1,128,28,28]> ⬅️ ::Conv(%\"relu_12\", %\"layer2.1.conv1.weight\"{...}, %\"layer2.1.conv1.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             34 |  # node_relu_13\n",
       "                   %\"relu_13\"<FLOAT,[1,128,28,28]> ⬅️ ::Relu(%\"getitem_45\")\n",
       "             35 |  # node_Conv_785\n",
       "                   %\"getitem_48\"<FLOAT,[1,128,28,28]> ⬅️ ::Conv(%\"relu_13\", %\"layer2.1.conv2.weight\"{...}, %\"layer2.1.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             36 |  # node_relu_14\n",
       "                   %\"relu_14\"<FLOAT,[1,128,28,28]> ⬅️ ::Relu(%\"getitem_48\")\n",
       "             37 |  # node_Conv_787\n",
       "                   %\"getitem_51\"<FLOAT,[1,512,28,28]> ⬅️ ::Conv(%\"relu_14\", %\"layer2.1.conv3.weight\"{...}, %\"layer2.1.conv3.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             38 |  # node_add_4\n",
       "                   %\"add_4\"<FLOAT,[1,512,28,28]> ⬅️ ::Add(%\"getitem_51\", %\"relu_12\")\n",
       "             39 |  # node_relu_15\n",
       "                   %\"relu_15\"<FLOAT,[1,512,28,28]> ⬅️ ::Relu(%\"add_4\")\n",
       "             40 |  # node_Conv_789\n",
       "                   %\"getitem_54\"<FLOAT,[1,128,28,28]> ⬅️ ::Conv(%\"relu_15\", %\"layer2.2.conv1.weight\"{...}, %\"layer2.2.conv1.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             41 |  # node_relu_16\n",
       "                   %\"relu_16\"<FLOAT,[1,128,28,28]> ⬅️ ::Relu(%\"getitem_54\")\n",
       "             42 |  # node_Conv_791\n",
       "                   %\"getitem_57\"<FLOAT,[1,128,28,28]> ⬅️ ::Conv(%\"relu_16\", %\"layer2.2.conv2.weight\"{...}, %\"layer2.2.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             43 |  # node_relu_17\n",
       "                   %\"relu_17\"<FLOAT,[1,128,28,28]> ⬅️ ::Relu(%\"getitem_57\")\n",
       "             44 |  # node_Conv_793\n",
       "                   %\"getitem_60\"<FLOAT,[1,512,28,28]> ⬅️ ::Conv(%\"relu_17\", %\"layer2.2.conv3.weight\"{...}, %\"layer2.2.conv3.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             45 |  # node_add_5\n",
       "                   %\"add_5\"<FLOAT,[1,512,28,28]> ⬅️ ::Add(%\"getitem_60\", %\"relu_15\")\n",
       "             46 |  # node_relu_18\n",
       "                   %\"relu_18\"<FLOAT,[1,512,28,28]> ⬅️ ::Relu(%\"add_5\")\n",
       "             47 |  # node_Conv_795\n",
       "                   %\"getitem_63\"<FLOAT,[1,128,28,28]> ⬅️ ::Conv(%\"relu_18\", %\"layer2.3.conv1.weight\"{...}, %\"layer2.3.conv1.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             48 |  # node_relu_19\n",
       "                   %\"relu_19\"<FLOAT,[1,128,28,28]> ⬅️ ::Relu(%\"getitem_63\")\n",
       "             49 |  # node_Conv_797\n",
       "                   %\"getitem_66\"<FLOAT,[1,128,28,28]> ⬅️ ::Conv(%\"relu_19\", %\"layer2.3.conv2.weight\"{...}, %\"layer2.3.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             50 |  # node_relu_20\n",
       "                   %\"relu_20\"<FLOAT,[1,128,28,28]> ⬅️ ::Relu(%\"getitem_66\")\n",
       "             51 |  # node_Conv_799\n",
       "                   %\"getitem_69\"<FLOAT,[1,512,28,28]> ⬅️ ::Conv(%\"relu_20\", %\"layer2.3.conv3.weight\"{...}, %\"layer2.3.conv3.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             52 |  # node_add_6\n",
       "                   %\"add_6\"<FLOAT,[1,512,28,28]> ⬅️ ::Add(%\"getitem_69\", %\"relu_18\")\n",
       "             53 |  # node_relu_21\n",
       "                   %\"relu_21\"<FLOAT,[1,512,28,28]> ⬅️ ::Relu(%\"add_6\")\n",
       "             54 |  # node_Conv_801\n",
       "                   %\"getitem_72\"<FLOAT,[1,256,28,28]> ⬅️ ::Conv(%\"relu_21\", %\"layer3.0.conv1.weight\"{...}, %\"layer3.0.conv1.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             55 |  # node_relu_22\n",
       "                   %\"relu_22\"<FLOAT,[1,256,28,28]> ⬅️ ::Relu(%\"getitem_72\")\n",
       "             56 |  # node_Conv_803\n",
       "                   %\"getitem_75\"<FLOAT,[1,256,14,14]> ⬅️ ::Conv(%\"relu_22\", %\"layer3.0.conv2.weight\"{...}, %\"layer3.0.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "             57 |  # node_relu_23\n",
       "                   %\"relu_23\"<FLOAT,[1,256,14,14]> ⬅️ ::Relu(%\"getitem_75\")\n",
       "             58 |  # node_Conv_805\n",
       "                   %\"getitem_78\"<FLOAT,[1,1024,14,14]> ⬅️ ::Conv(%\"relu_23\", %\"layer3.0.conv3.weight\"{...}, %\"layer3.0.conv3.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             59 |  # node_Conv_807\n",
       "                   %\"getitem_81\"<FLOAT,[1,1024,14,14]> ⬅️ ::Conv(%\"relu_21\", %\"layer3.0.downsample.0.weight\"{...}, %\"layer3.0.downsample.0.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "             60 |  # node_add_7\n",
       "                   %\"add_7\"<FLOAT,[1,1024,14,14]> ⬅️ ::Add(%\"getitem_78\", %\"getitem_81\")\n",
       "             61 |  # node_relu_24\n",
       "                   %\"relu_24\"<FLOAT,[1,1024,14,14]> ⬅️ ::Relu(%\"add_7\")\n",
       "             62 |  # node_Conv_809\n",
       "                   %\"getitem_84\"<FLOAT,[1,256,14,14]> ⬅️ ::Conv(%\"relu_24\", %\"layer3.1.conv1.weight\"{...}, %\"layer3.1.conv1.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             63 |  # node_relu_25\n",
       "                   %\"relu_25\"<FLOAT,[1,256,14,14]> ⬅️ ::Relu(%\"getitem_84\")\n",
       "             64 |  # node_Conv_811\n",
       "                   %\"getitem_87\"<FLOAT,[1,256,14,14]> ⬅️ ::Conv(%\"relu_25\", %\"layer3.1.conv2.weight\"{...}, %\"layer3.1.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             65 |  # node_relu_26\n",
       "                   %\"relu_26\"<FLOAT,[1,256,14,14]> ⬅️ ::Relu(%\"getitem_87\")\n",
       "             66 |  # node_Conv_813\n",
       "                   %\"getitem_90\"<FLOAT,[1,1024,14,14]> ⬅️ ::Conv(%\"relu_26\", %\"layer3.1.conv3.weight\"{...}, %\"layer3.1.conv3.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             67 |  # node_add_8\n",
       "                   %\"add_8\"<FLOAT,[1,1024,14,14]> ⬅️ ::Add(%\"getitem_90\", %\"relu_24\")\n",
       "             68 |  # node_relu_27\n",
       "                   %\"relu_27\"<FLOAT,[1,1024,14,14]> ⬅️ ::Relu(%\"add_8\")\n",
       "             69 |  # node_Conv_815\n",
       "                   %\"getitem_93\"<FLOAT,[1,256,14,14]> ⬅️ ::Conv(%\"relu_27\", %\"layer3.2.conv1.weight\"{...}, %\"layer3.2.conv1.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             70 |  # node_relu_28\n",
       "                   %\"relu_28\"<FLOAT,[1,256,14,14]> ⬅️ ::Relu(%\"getitem_93\")\n",
       "             71 |  # node_Conv_817\n",
       "                   %\"getitem_96\"<FLOAT,[1,256,14,14]> ⬅️ ::Conv(%\"relu_28\", %\"layer3.2.conv2.weight\"{...}, %\"layer3.2.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             72 |  # node_relu_29\n",
       "                   %\"relu_29\"<FLOAT,[1,256,14,14]> ⬅️ ::Relu(%\"getitem_96\")\n",
       "             73 |  # node_Conv_819\n",
       "                   %\"getitem_99\"<FLOAT,[1,1024,14,14]> ⬅️ ::Conv(%\"relu_29\", %\"layer3.2.conv3.weight\"{...}, %\"layer3.2.conv3.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             74 |  # node_add_9\n",
       "                   %\"add_9\"<FLOAT,[1,1024,14,14]> ⬅️ ::Add(%\"getitem_99\", %\"relu_27\")\n",
       "             75 |  # node_relu_30\n",
       "                   %\"relu_30\"<FLOAT,[1,1024,14,14]> ⬅️ ::Relu(%\"add_9\")\n",
       "             76 |  # node_Conv_821\n",
       "                   %\"getitem_102\"<FLOAT,[1,256,14,14]> ⬅️ ::Conv(%\"relu_30\", %\"layer3.3.conv1.weight\"{...}, %\"layer3.3.conv1.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             77 |  # node_relu_31\n",
       "                   %\"relu_31\"<FLOAT,[1,256,14,14]> ⬅️ ::Relu(%\"getitem_102\")\n",
       "             78 |  # node_Conv_823\n",
       "                   %\"getitem_105\"<FLOAT,[1,256,14,14]> ⬅️ ::Conv(%\"relu_31\", %\"layer3.3.conv2.weight\"{...}, %\"layer3.3.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             79 |  # node_relu_32\n",
       "                   %\"relu_32\"<FLOAT,[1,256,14,14]> ⬅️ ::Relu(%\"getitem_105\")\n",
       "             80 |  # node_Conv_825\n",
       "                   %\"getitem_108\"<FLOAT,[1,1024,14,14]> ⬅️ ::Conv(%\"relu_32\", %\"layer3.3.conv3.weight\"{...}, %\"layer3.3.conv3.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             81 |  # node_add_10\n",
       "                   %\"add_10\"<FLOAT,[1,1024,14,14]> ⬅️ ::Add(%\"getitem_108\", %\"relu_30\")\n",
       "             82 |  # node_relu_33\n",
       "                   %\"relu_33\"<FLOAT,[1,1024,14,14]> ⬅️ ::Relu(%\"add_10\")\n",
       "             83 |  # node_Conv_827\n",
       "                   %\"getitem_111\"<FLOAT,[1,256,14,14]> ⬅️ ::Conv(%\"relu_33\", %\"layer3.4.conv1.weight\"{...}, %\"layer3.4.conv1.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             84 |  # node_relu_34\n",
       "                   %\"relu_34\"<FLOAT,[1,256,14,14]> ⬅️ ::Relu(%\"getitem_111\")\n",
       "             85 |  # node_Conv_829\n",
       "                   %\"getitem_114\"<FLOAT,[1,256,14,14]> ⬅️ ::Conv(%\"relu_34\", %\"layer3.4.conv2.weight\"{...}, %\"layer3.4.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             86 |  # node_relu_35\n",
       "                   %\"relu_35\"<FLOAT,[1,256,14,14]> ⬅️ ::Relu(%\"getitem_114\")\n",
       "             87 |  # node_Conv_831\n",
       "                   %\"getitem_117\"<FLOAT,[1,1024,14,14]> ⬅️ ::Conv(%\"relu_35\", %\"layer3.4.conv3.weight\"{...}, %\"layer3.4.conv3.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             88 |  # node_add_11\n",
       "                   %\"add_11\"<FLOAT,[1,1024,14,14]> ⬅️ ::Add(%\"getitem_117\", %\"relu_33\")\n",
       "             89 |  # node_relu_36\n",
       "                   %\"relu_36\"<FLOAT,[1,1024,14,14]> ⬅️ ::Relu(%\"add_11\")\n",
       "             90 |  # node_Conv_833\n",
       "                   %\"getitem_120\"<FLOAT,[1,256,14,14]> ⬅️ ::Conv(%\"relu_36\", %\"layer3.5.conv1.weight\"{...}, %\"layer3.5.conv1.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             91 |  # node_relu_37\n",
       "                   %\"relu_37\"<FLOAT,[1,256,14,14]> ⬅️ ::Relu(%\"getitem_120\")\n",
       "             92 |  # node_Conv_835\n",
       "                   %\"getitem_123\"<FLOAT,[1,256,14,14]> ⬅️ ::Conv(%\"relu_37\", %\"layer3.5.conv2.weight\"{...}, %\"layer3.5.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             93 |  # node_relu_38\n",
       "                   %\"relu_38\"<FLOAT,[1,256,14,14]> ⬅️ ::Relu(%\"getitem_123\")\n",
       "             94 |  # node_Conv_837\n",
       "                   %\"getitem_126\"<FLOAT,[1,1024,14,14]> ⬅️ ::Conv(%\"relu_38\", %\"layer3.5.conv3.weight\"{...}, %\"layer3.5.conv3.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             95 |  # node_add_12\n",
       "                   %\"add_12\"<FLOAT,[1,1024,14,14]> ⬅️ ::Add(%\"getitem_126\", %\"relu_36\")\n",
       "             96 |  # node_relu_39\n",
       "                   %\"relu_39\"<FLOAT,[1,1024,14,14]> ⬅️ ::Relu(%\"add_12\")\n",
       "             97 |  # node_Conv_839\n",
       "                   %\"getitem_129\"<FLOAT,[1,512,14,14]> ⬅️ ::Conv(%\"relu_39\", %\"layer4.0.conv1.weight\"{...}, %\"layer4.0.conv1.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "             98 |  # node_relu_40\n",
       "                   %\"relu_40\"<FLOAT,[1,512,14,14]> ⬅️ ::Relu(%\"getitem_129\")\n",
       "             99 |  # node_Conv_841\n",
       "                   %\"getitem_132\"<FLOAT,[1,512,7,7]> ⬅️ ::Conv(%\"relu_40\", %\"layer4.0.conv2.weight\"{...}, %\"layer4.0.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "            100 |  # node_relu_41\n",
       "                   %\"relu_41\"<FLOAT,[1,512,7,7]> ⬅️ ::Relu(%\"getitem_132\")\n",
       "            101 |  # node_Conv_843\n",
       "                   %\"getitem_135\"<FLOAT,[1,2048,7,7]> ⬅️ ::Conv(%\"relu_41\", %\"layer4.0.conv3.weight\"{...}, %\"layer4.0.conv3.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            102 |  # node_Conv_845\n",
       "                   %\"getitem_138\"<FLOAT,[1,2048,7,7]> ⬅️ ::Conv(%\"relu_39\", %\"layer4.0.downsample.0.weight\"{...}, %\"layer4.0.downsample.0.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "            103 |  # node_add_13\n",
       "                   %\"add_13\"<FLOAT,[1,2048,7,7]> ⬅️ ::Add(%\"getitem_135\", %\"getitem_138\")\n",
       "            104 |  # node_relu_42\n",
       "                   %\"relu_42\"<FLOAT,[1,2048,7,7]> ⬅️ ::Relu(%\"add_13\")\n",
       "            105 |  # node_Conv_847\n",
       "                   %\"getitem_141\"<FLOAT,[1,512,7,7]> ⬅️ ::Conv(%\"relu_42\", %\"layer4.1.conv1.weight\"{...}, %\"layer4.1.conv1.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            106 |  # node_relu_43\n",
       "                   %\"relu_43\"<FLOAT,[1,512,7,7]> ⬅️ ::Relu(%\"getitem_141\")\n",
       "            107 |  # node_Conv_849\n",
       "                   %\"getitem_144\"<FLOAT,[1,512,7,7]> ⬅️ ::Conv(%\"relu_43\", %\"layer4.1.conv2.weight\"{...}, %\"layer4.1.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            108 |  # node_relu_44\n",
       "                   %\"relu_44\"<FLOAT,[1,512,7,7]> ⬅️ ::Relu(%\"getitem_144\")\n",
       "            109 |  # node_Conv_851\n",
       "                   %\"getitem_147\"<FLOAT,[1,2048,7,7]> ⬅️ ::Conv(%\"relu_44\", %\"layer4.1.conv3.weight\"{...}, %\"layer4.1.conv3.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            110 |  # node_add_14\n",
       "                   %\"add_14\"<FLOAT,[1,2048,7,7]> ⬅️ ::Add(%\"getitem_147\", %\"relu_42\")\n",
       "            111 |  # node_relu_45\n",
       "                   %\"relu_45\"<FLOAT,[1,2048,7,7]> ⬅️ ::Relu(%\"add_14\")\n",
       "            112 |  # node_Conv_853\n",
       "                   %\"getitem_150\"<FLOAT,[1,512,7,7]> ⬅️ ::Conv(%\"relu_45\", %\"layer4.2.conv1.weight\"{...}, %\"layer4.2.conv1.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            113 |  # node_relu_46\n",
       "                   %\"relu_46\"<FLOAT,[1,512,7,7]> ⬅️ ::Relu(%\"getitem_150\")\n",
       "            114 |  # node_Conv_855\n",
       "                   %\"getitem_153\"<FLOAT,[1,512,7,7]> ⬅️ ::Conv(%\"relu_46\", %\"layer4.2.conv2.weight\"{...}, %\"layer4.2.conv2.weight_bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            115 |  # node_relu_47\n",
       "                   %\"relu_47\"<FLOAT,[1,512,7,7]> ⬅️ ::Relu(%\"getitem_153\")\n",
       "            116 |  # node_Conv_857\n",
       "                   %\"getitem_156\"<FLOAT,[1,2048,7,7]> ⬅️ ::Conv(%\"relu_47\", %\"layer4.2.conv3.weight\"{...}, %\"layer4.2.conv3.weight_bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            117 |  # node_add_15\n",
       "                   %\"add_15\"<FLOAT,[1,2048,7,7]> ⬅️ ::Add(%\"getitem_156\", %\"relu_45\")\n",
       "            118 |  # node_relu_48\n",
       "                   %\"relu_48\"<FLOAT,[1,2048,7,7]> ⬅️ ::Relu(%\"add_15\")\n",
       "            119 |  # node_mean\n",
       "                   %\"mean\"<FLOAT,[1,2048,1,1]> ⬅️ ::ReduceMean(%\"relu_48\", %\"val_589\"{[-1, -2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "            120 |  # node_view\n",
       "                   %\"view\"<FLOAT,[1,2048]> ⬅️ ::Reshape(%\"mean\", %\"val_593\"{[1, 2048]}) {allowzero=1}\n",
       "            121 |  # node_linear\n",
       "                   %\"output\"<FLOAT,[1,1000]> ⬅️ ::Gemm(%\"view\", %\"fc.weight\"{...}, %\"fc.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            return %\"output\"<FLOAT,[1,1000]>\n",
       "        }\n",
       "\n",
       "\n",
       "    ,\n",
       "    exported_program=\n",
       "        ExportedProgram:\n",
       "            class GraphModule(torch.nn.Module):\n",
       "                def forward(self, p_conv1_weight: \"f32[64, 3, 7, 7]\", p_bn1_weight: \"f32[64]\", p_bn1_bias: \"f32[64]\", p_layer1_0_conv1_weight: \"f32[64, 64, 1, 1]\", p_layer1_0_bn1_weight: \"f32[64]\", p_layer1_0_bn1_bias: \"f32[64]\", p_layer1_0_conv2_weight: \"f32[64, 64, 3, 3]\", p_layer1_0_bn2_weight: \"f32[64]\", p_layer1_0_bn2_bias: \"f32[64]\", p_layer1_0_conv3_weight: \"f32[256, 64, 1, 1]\", p_layer1_0_bn3_weight: \"f32[256]\", p_layer1_0_bn3_bias: \"f32[256]\", p_layer1_0_downsample_0_weight: \"f32[256, 64, 1, 1]\", p_layer1_0_downsample_1_weight: \"f32[256]\", p_layer1_0_downsample_1_bias: \"f32[256]\", p_layer1_1_conv1_weight: \"f32[64, 256, 1, 1]\", p_layer1_1_bn1_weight: \"f32[64]\", p_layer1_1_bn1_bias: \"f32[64]\", p_layer1_1_conv2_weight: \"f32[64, 64, 3, 3]\", p_layer1_1_bn2_weight: \"f32[64]\", p_layer1_1_bn2_bias: \"f32[64]\", p_layer1_1_conv3_weight: \"f32[256, 64, 1, 1]\", p_layer1_1_bn3_weight: \"f32[256]\", p_layer1_1_bn3_bias: \"f32[256]\", p_layer1_2_conv1_weight: \"f32[64, 256, 1, 1]\", p_layer1_2_bn1_weight: \"f32[64]\", p_layer1_2_bn1_bias: \"f32[64]\", p_layer1_2_conv2_weight: \"f32[64, 64, 3, 3]\", p_layer1_2_bn2_weight: \"f32[64]\", p_layer1_2_bn2_bias: \"f32[64]\", p_layer1_2_conv3_weight: \"f32[256, 64, 1, 1]\", p_layer1_2_bn3_weight: \"f32[256]\", p_layer1_2_bn3_bias: \"f32[256]\", p_layer2_0_conv1_weight: \"f32[128, 256, 1, 1]\", p_layer2_0_bn1_weight: \"f32[128]\", p_layer2_0_bn1_bias: \"f32[128]\", p_layer2_0_conv2_weight: \"f32[128, 128, 3, 3]\", p_layer2_0_bn2_weight: \"f32[128]\", p_layer2_0_bn2_bias: \"f32[128]\", p_layer2_0_conv3_weight: \"f32[512, 128, 1, 1]\", p_layer2_0_bn3_weight: \"f32[512]\", p_layer2_0_bn3_bias: \"f32[512]\", p_layer2_0_downsample_0_weight: \"f32[512, 256, 1, 1]\", p_layer2_0_downsample_1_weight: \"f32[512]\", p_layer2_0_downsample_1_bias: \"f32[512]\", p_layer2_1_conv1_weight: \"f32[128, 512, 1, 1]\", p_layer2_1_bn1_weight: \"f32[128]\", p_layer2_1_bn1_bias: \"f32[128]\", p_layer2_1_conv2_weight: \"f32[128, 128, 3, 3]\", p_layer2_1_bn2_weight: \"f32[128]\", p_layer2_1_bn2_bias: \"f32[128]\", p_layer2_1_conv3_weight: \"f32[512, 128, 1, 1]\", p_layer2_1_bn3_weight: \"f32[512]\", p_layer2_1_bn3_bias: \"f32[512]\", p_layer2_2_conv1_weight: \"f32[128, 512, 1, 1]\", p_layer2_2_bn1_weight: \"f32[128]\", p_layer2_2_bn1_bias: \"f32[128]\", p_layer2_2_conv2_weight: \"f32[128, 128, 3, 3]\", p_layer2_2_bn2_weight: \"f32[128]\", p_layer2_2_bn2_bias: \"f32[128]\", p_layer2_2_conv3_weight: \"f32[512, 128, 1, 1]\", p_layer2_2_bn3_weight: \"f32[512]\", p_layer2_2_bn3_bias: \"f32[512]\", p_layer2_3_conv1_weight: \"f32[128, 512, 1, 1]\", p_layer2_3_bn1_weight: \"f32[128]\", p_layer2_3_bn1_bias: \"f32[128]\", p_layer2_3_conv2_weight: \"f32[128, 128, 3, 3]\", p_layer2_3_bn2_weight: \"f32[128]\", p_layer2_3_bn2_bias: \"f32[128]\", p_layer2_3_conv3_weight: \"f32[512, 128, 1, 1]\", p_layer2_3_bn3_weight: \"f32[512]\", p_layer2_3_bn3_bias: \"f32[512]\", p_layer3_0_conv1_weight: \"f32[256, 512, 1, 1]\", p_layer3_0_bn1_weight: \"f32[256]\", p_layer3_0_bn1_bias: \"f32[256]\", p_layer3_0_conv2_weight: \"f32[256, 256, 3, 3]\", p_layer3_0_bn2_weight: \"f32[256]\", p_layer3_0_bn2_bias: \"f32[256]\", p_layer3_0_conv3_weight: \"f32[1024, 256, 1, 1]\", p_layer3_0_bn3_weight: \"f32[1024]\", p_layer3_0_bn3_bias: \"f32[1024]\", p_layer3_0_downsample_0_weight: \"f32[1024, 512, 1, 1]\", p_layer3_0_downsample_1_weight: \"f32[1024]\", p_layer3_0_downsample_1_bias: \"f32[1024]\", p_layer3_1_conv1_weight: \"f32[256, 1024, 1, 1]\", p_layer3_1_bn1_weight: \"f32[256]\", p_layer3_1_bn1_bias: \"f32[256]\", p_layer3_1_conv2_weight: \"f32[256, 256, 3, 3]\", p_layer3_1_bn2_weight: \"f32[256]\", p_layer3_1_bn2_bias: \"f32[256]\", p_layer3_1_conv3_weight: \"f32[1024, 256, 1, 1]\", p_layer3_1_bn3_weight: \"f32[1024]\", p_layer3_1_bn3_bias: \"f32[1024]\", p_layer3_2_conv1_weight: \"f32[256, 1024, 1, 1]\", p_layer3_2_bn1_weight: \"f32[256]\", p_layer3_2_bn1_bias: \"f32[256]\", p_layer3_2_conv2_weight: \"f32[256, 256, 3, 3]\", p_layer3_2_bn2_weight: \"f32[256]\", p_layer3_2_bn2_bias: \"f32[256]\", p_layer3_2_conv3_weight: \"f32[1024, 256, 1, 1]\", p_layer3_2_bn3_weight: \"f32[1024]\", p_layer3_2_bn3_bias: \"f32[1024]\", p_layer3_3_conv1_weight: \"f32[256, 1024, 1, 1]\", p_layer3_3_bn1_weight: \"f32[256]\", p_layer3_3_bn1_bias: \"f32[256]\", p_layer3_3_conv2_weight: \"f32[256, 256, 3, 3]\", p_layer3_3_bn2_weight: \"f32[256]\", p_layer3_3_bn2_bias: \"f32[256]\", p_layer3_3_conv3_weight: \"f32[1024, 256, 1, 1]\", p_layer3_3_bn3_weight: \"f32[1024]\", p_layer3_3_bn3_bias: \"f32[1024]\", p_layer3_4_conv1_weight: \"f32[256, 1024, 1, 1]\", p_layer3_4_bn1_weight: \"f32[256]\", p_layer3_4_bn1_bias: \"f32[256]\", p_layer3_4_conv2_weight: \"f32[256, 256, 3, 3]\", p_layer3_4_bn2_weight: \"f32[256]\", p_layer3_4_bn2_bias: \"f32[256]\", p_layer3_4_conv3_weight: \"f32[1024, 256, 1, 1]\", p_layer3_4_bn3_weight: \"f32[1024]\", p_layer3_4_bn3_bias: \"f32[1024]\", p_layer3_5_conv1_weight: \"f32[256, 1024, 1, 1]\", p_layer3_5_bn1_weight: \"f32[256]\", p_layer3_5_bn1_bias: \"f32[256]\", p_layer3_5_conv2_weight: \"f32[256, 256, 3, 3]\", p_layer3_5_bn2_weight: \"f32[256]\", p_layer3_5_bn2_bias: \"f32[256]\", p_layer3_5_conv3_weight: \"f32[1024, 256, 1, 1]\", p_layer3_5_bn3_weight: \"f32[1024]\", p_layer3_5_bn3_bias: \"f32[1024]\", p_layer4_0_conv1_weight: \"f32[512, 1024, 1, 1]\", p_layer4_0_bn1_weight: \"f32[512]\", p_layer4_0_bn1_bias: \"f32[512]\", p_layer4_0_conv2_weight: \"f32[512, 512, 3, 3]\", p_layer4_0_bn2_weight: \"f32[512]\", p_layer4_0_bn2_bias: \"f32[512]\", p_layer4_0_conv3_weight: \"f32[2048, 512, 1, 1]\", p_layer4_0_bn3_weight: \"f32[2048]\", p_layer4_0_bn3_bias: \"f32[2048]\", p_layer4_0_downsample_0_weight: \"f32[2048, 1024, 1, 1]\", p_layer4_0_downsample_1_weight: \"f32[2048]\", p_layer4_0_downsample_1_bias: \"f32[2048]\", p_layer4_1_conv1_weight: \"f32[512, 2048, 1, 1]\", p_layer4_1_bn1_weight: \"f32[512]\", p_layer4_1_bn1_bias: \"f32[512]\", p_layer4_1_conv2_weight: \"f32[512, 512, 3, 3]\", p_layer4_1_bn2_weight: \"f32[512]\", p_layer4_1_bn2_bias: \"f32[512]\", p_layer4_1_conv3_weight: \"f32[2048, 512, 1, 1]\", p_layer4_1_bn3_weight: \"f32[2048]\", p_layer4_1_bn3_bias: \"f32[2048]\", p_layer4_2_conv1_weight: \"f32[512, 2048, 1, 1]\", p_layer4_2_bn1_weight: \"f32[512]\", p_layer4_2_bn1_bias: \"f32[512]\", p_layer4_2_conv2_weight: \"f32[512, 512, 3, 3]\", p_layer4_2_bn2_weight: \"f32[512]\", p_layer4_2_bn2_bias: \"f32[512]\", p_layer4_2_conv3_weight: \"f32[2048, 512, 1, 1]\", p_layer4_2_bn3_weight: \"f32[2048]\", p_layer4_2_bn3_bias: \"f32[2048]\", p_fc_weight: \"f32[1000, 2048]\", p_fc_bias: \"f32[1000]\", b_bn1_running_mean: \"f32[64]\", b_bn1_running_var: \"f32[64]\", b_bn1_num_batches_tracked: \"i64[]\", b_layer1_0_bn1_running_mean: \"f32[64]\", b_layer1_0_bn1_running_var: \"f32[64]\", b_layer1_0_bn1_num_batches_tracked: \"i64[]\", b_layer1_0_bn2_running_mean: \"f32[64]\", b_layer1_0_bn2_running_var: \"f32[64]\", b_layer1_0_bn2_num_batches_tracked: \"i64[]\", b_layer1_0_bn3_running_mean: \"f32[256]\", b_layer1_0_bn3_running_var: \"f32[256]\", b_layer1_0_bn3_num_batches_tracked: \"i64[]\", b_layer1_0_downsample_1_running_mean: \"f32[256]\", b_layer1_0_downsample_1_running_var: \"f32[256]\", b_layer1_0_downsample_1_num_batches_tracked: \"i64[]\", b_layer1_1_bn1_running_mean: \"f32[64]\", b_layer1_1_bn1_running_var: \"f32[64]\", b_layer1_1_bn1_num_batches_tracked: \"i64[]\", b_layer1_1_bn2_running_mean: \"f32[64]\", b_layer1_1_bn2_running_var: \"f32[64]\", b_layer1_1_bn2_num_batches_tracked: \"i64[]\", b_layer1_1_bn3_running_mean: \"f32[256]\", b_layer1_1_bn3_running_var: \"f32[256]\", b_layer1_1_bn3_num_batches_tracked: \"i64[]\", b_layer1_2_bn1_running_mean: \"f32[64]\", b_layer1_2_bn1_running_var: \"f32[64]\", b_layer1_2_bn1_num_batches_tracked: \"i64[]\", b_layer1_2_bn2_running_mean: \"f32[64]\", b_layer1_2_bn2_running_var: \"f32[64]\", b_layer1_2_bn2_num_batches_tracked: \"i64[]\", b_layer1_2_bn3_running_mean: \"f32[256]\", b_layer1_2_bn3_running_var: \"f32[256]\", b_layer1_2_bn3_num_batches_tracked: \"i64[]\", b_layer2_0_bn1_running_mean: \"f32[128]\", b_layer2_0_bn1_running_var: \"f32[128]\", b_layer2_0_bn1_num_batches_tracked: \"i64[]\", b_layer2_0_bn2_running_mean: \"f32[128]\", b_layer2_0_bn2_running_var: \"f32[128]\", b_layer2_0_bn2_num_batches_tracked: \"i64[]\", b_layer2_0_bn3_running_mean: \"f32[512]\", b_layer2_0_bn3_running_var: \"f32[512]\", b_layer2_0_bn3_num_batches_tracked: \"i64[]\", b_layer2_0_downsample_1_running_mean: \"f32[512]\", b_layer2_0_downsample_1_running_var: \"f32[512]\", b_layer2_0_downsample_1_num_batches_tracked: \"i64[]\", b_layer2_1_bn1_running_mean: \"f32[128]\", b_layer2_1_bn1_running_var: \"f32[128]\", b_layer2_1_bn1_num_batches_tracked: \"i64[]\", b_layer2_1_bn2_running_mean: \"f32[128]\", b_layer2_1_bn2_running_var: \"f32[128]\", b_layer2_1_bn2_num_batches_tracked: \"i64[]\", b_layer2_1_bn3_running_mean: \"f32[512]\", b_layer2_1_bn3_running_var: \"f32[512]\", b_layer2_1_bn3_num_batches_tracked: \"i64[]\", b_layer2_2_bn1_running_mean: \"f32[128]\", b_layer2_2_bn1_running_var: \"f32[128]\", b_layer2_2_bn1_num_batches_tracked: \"i64[]\", b_layer2_2_bn2_running_mean: \"f32[128]\", b_layer2_2_bn2_running_var: \"f32[128]\", b_layer2_2_bn2_num_batches_tracked: \"i64[]\", b_layer2_2_bn3_running_mean: \"f32[512]\", b_layer2_2_bn3_running_var: \"f32[512]\", b_layer2_2_bn3_num_batches_tracked: \"i64[]\", b_layer2_3_bn1_running_mean: \"f32[128]\", b_layer2_3_bn1_running_var: \"f32[128]\", b_layer2_3_bn1_num_batches_tracked: \"i64[]\", b_layer2_3_bn2_running_mean: \"f32[128]\", b_layer2_3_bn2_running_var: \"f32[128]\", b_layer2_3_bn2_num_batches_tracked: \"i64[]\", b_layer2_3_bn3_running_mean: \"f32[512]\", b_layer2_3_bn3_running_var: \"f32[512]\", b_layer2_3_bn3_num_batches_tracked: \"i64[]\", b_layer3_0_bn1_running_mean: \"f32[256]\", b_layer3_0_bn1_running_var: \"f32[256]\", b_layer3_0_bn1_num_batches_tracked: \"i64[]\", b_layer3_0_bn2_running_mean: \"f32[256]\", b_layer3_0_bn2_running_var: \"f32[256]\", b_layer3_0_bn2_num_batches_tracked: \"i64[]\", b_layer3_0_bn3_running_mean: \"f32[1024]\", b_layer3_0_bn3_running_var: \"f32[1024]\", b_layer3_0_bn3_num_batches_tracked: \"i64[]\", b_layer3_0_downsample_1_running_mean: \"f32[1024]\", b_layer3_0_downsample_1_running_var: \"f32[1024]\", b_layer3_0_downsample_1_num_batches_tracked: \"i64[]\", b_layer3_1_bn1_running_mean: \"f32[256]\", b_layer3_1_bn1_running_var: \"f32[256]\", b_layer3_1_bn1_num_batches_tracked: \"i64[]\", b_layer3_1_bn2_running_mean: \"f32[256]\", b_layer3_1_bn2_running_var: \"f32[256]\", b_layer3_1_bn2_num_batches_tracked: \"i64[]\", b_layer3_1_bn3_running_mean: \"f32[1024]\", b_layer3_1_bn3_running_var: \"f32[1024]\", b_layer3_1_bn3_num_batches_tracked: \"i64[]\", b_layer3_2_bn1_running_mean: \"f32[256]\", b_layer3_2_bn1_running_var: \"f32[256]\", b_layer3_2_bn1_num_batches_tracked: \"i64[]\", b_layer3_2_bn2_running_mean: \"f32[256]\", b_layer3_2_bn2_running_var: \"f32[256]\", b_layer3_2_bn2_num_batches_tracked: \"i64[]\", b_layer3_2_bn3_running_mean: \"f32[1024]\", b_layer3_2_bn3_running_var: \"f32[1024]\", b_layer3_2_bn3_num_batches_tracked: \"i64[]\", b_layer3_3_bn1_running_mean: \"f32[256]\", b_layer3_3_bn1_running_var: \"f32[256]\", b_layer3_3_bn1_num_batches_tracked: \"i64[]\", b_layer3_3_bn2_running_mean: \"f32[256]\", b_layer3_3_bn2_running_var: \"f32[256]\", b_layer3_3_bn2_num_batches_tracked: \"i64[]\", b_layer3_3_bn3_running_mean: \"f32[1024]\", b_layer3_3_bn3_running_var: \"f32[1024]\", b_layer3_3_bn3_num_batches_tracked: \"i64[]\", b_layer3_4_bn1_running_mean: \"f32[256]\", b_layer3_4_bn1_running_var: \"f32[256]\", b_layer3_4_bn1_num_batches_tracked: \"i64[]\", b_layer3_4_bn2_running_mean: \"f32[256]\", b_layer3_4_bn2_running_var: \"f32[256]\", b_layer3_4_bn2_num_batches_tracked: \"i64[]\", b_layer3_4_bn3_running_mean: \"f32[1024]\", b_layer3_4_bn3_running_var: \"f32[1024]\", b_layer3_4_bn3_num_batches_tracked: \"i64[]\", b_layer3_5_bn1_running_mean: \"f32[256]\", b_layer3_5_bn1_running_var: \"f32[256]\", b_layer3_5_bn1_num_batches_tracked: \"i64[]\", b_layer3_5_bn2_running_mean: \"f32[256]\", b_layer3_5_bn2_running_var: \"f32[256]\", b_layer3_5_bn2_num_batches_tracked: \"i64[]\", b_layer3_5_bn3_running_mean: \"f32[1024]\", b_layer3_5_bn3_running_var: \"f32[1024]\", b_layer3_5_bn3_num_batches_tracked: \"i64[]\", b_layer4_0_bn1_running_mean: \"f32[512]\", b_layer4_0_bn1_running_var: \"f32[512]\", b_layer4_0_bn1_num_batches_tracked: \"i64[]\", b_layer4_0_bn2_running_mean: \"f32[512]\", b_layer4_0_bn2_running_var: \"f32[512]\", b_layer4_0_bn2_num_batches_tracked: \"i64[]\", b_layer4_0_bn3_running_mean: \"f32[2048]\", b_layer4_0_bn3_running_var: \"f32[2048]\", b_layer4_0_bn3_num_batches_tracked: \"i64[]\", b_layer4_0_downsample_1_running_mean: \"f32[2048]\", b_layer4_0_downsample_1_running_var: \"f32[2048]\", b_layer4_0_downsample_1_num_batches_tracked: \"i64[]\", b_layer4_1_bn1_running_mean: \"f32[512]\", b_layer4_1_bn1_running_var: \"f32[512]\", b_layer4_1_bn1_num_batches_tracked: \"i64[]\", b_layer4_1_bn2_running_mean: \"f32[512]\", b_layer4_1_bn2_running_var: \"f32[512]\", b_layer4_1_bn2_num_batches_tracked: \"i64[]\", b_layer4_1_bn3_running_mean: \"f32[2048]\", b_layer4_1_bn3_running_var: \"f32[2048]\", b_layer4_1_bn3_num_batches_tracked: \"i64[]\", b_layer4_2_bn1_running_mean: \"f32[512]\", b_layer4_2_bn1_running_var: \"f32[512]\", b_layer4_2_bn1_num_batches_tracked: \"i64[]\", b_layer4_2_bn2_running_mean: \"f32[512]\", b_layer4_2_bn2_running_var: \"f32[512]\", b_layer4_2_bn2_num_batches_tracked: \"i64[]\", b_layer4_2_bn3_running_mean: \"f32[2048]\", b_layer4_2_bn3_running_var: \"f32[2048]\", b_layer4_2_bn3_num_batches_tracked: \"i64[]\", x: \"f32[1, 3, 224, 224]\"):\n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d: \"f32[1, 64, 112, 112]\" = torch.ops.aten.conv2d.default(x, p_conv1_weight, None, [2, 2], [3, 3]);  x = p_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d, p_bn1_weight, p_bn1_bias, b_bn1_running_mean, b_bn1_running_var, 0.1, 1e-05);  conv2d = p_bn1_weight = p_bn1_bias = b_bn1_running_mean = b_bn1_running_var = None\n",
       "                    getitem: \"f32[1, 64, 112, 112]\" = _native_batch_norm_legit_no_training[0];  _native_batch_norm_legit_no_training = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu: \"f32[1, 64, 112, 112]\" = torch.ops.aten.relu.default(getitem);  getitem = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/pooling.py:224 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d: \"f32[1, 64, 56, 56]\" = torch.ops.aten.max_pool2d.default(relu, [3, 3], [2, 2], [1, 1]);  relu = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_1: \"f32[1, 64, 56, 56]\" = torch.ops.aten.conv2d.default(max_pool2d, p_layer1_0_conv1_weight);  p_layer1_0_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_1 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_1, p_layer1_0_bn1_weight, p_layer1_0_bn1_bias, b_layer1_0_bn1_running_mean, b_layer1_0_bn1_running_var, 0.1, 1e-05);  conv2d_1 = p_layer1_0_bn1_weight = p_layer1_0_bn1_bias = b_layer1_0_bn1_running_mean = b_layer1_0_bn1_running_var = None\n",
       "                    getitem_3: \"f32[1, 64, 56, 56]\" = _native_batch_norm_legit_no_training_1[0];  _native_batch_norm_legit_no_training_1 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_1: \"f32[1, 64, 56, 56]\" = torch.ops.aten.relu.default(getitem_3);  getitem_3 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_2: \"f32[1, 64, 56, 56]\" = torch.ops.aten.conv2d.default(relu_1, p_layer1_0_conv2_weight, None, [1, 1], [1, 1]);  relu_1 = p_layer1_0_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_2 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_2, p_layer1_0_bn2_weight, p_layer1_0_bn2_bias, b_layer1_0_bn2_running_mean, b_layer1_0_bn2_running_var, 0.1, 1e-05);  conv2d_2 = p_layer1_0_bn2_weight = p_layer1_0_bn2_bias = b_layer1_0_bn2_running_mean = b_layer1_0_bn2_running_var = None\n",
       "                    getitem_6: \"f32[1, 64, 56, 56]\" = _native_batch_norm_legit_no_training_2[0];  _native_batch_norm_legit_no_training_2 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_2: \"f32[1, 64, 56, 56]\" = torch.ops.aten.relu.default(getitem_6);  getitem_6 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_3: \"f32[1, 256, 56, 56]\" = torch.ops.aten.conv2d.default(relu_2, p_layer1_0_conv3_weight);  relu_2 = p_layer1_0_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_3 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_3, p_layer1_0_bn3_weight, p_layer1_0_bn3_bias, b_layer1_0_bn3_running_mean, b_layer1_0_bn3_running_var, 0.1, 1e-05);  conv2d_3 = p_layer1_0_bn3_weight = p_layer1_0_bn3_bias = b_layer1_0_bn3_running_mean = b_layer1_0_bn3_running_var = None\n",
       "                    getitem_9: \"f32[1, 256, 56, 56]\" = _native_batch_norm_legit_no_training_3[0];  _native_batch_norm_legit_no_training_3 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_4: \"f32[1, 256, 56, 56]\" = torch.ops.aten.conv2d.default(max_pool2d, p_layer1_0_downsample_0_weight);  max_pool2d = p_layer1_0_downsample_0_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_4 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_4, p_layer1_0_downsample_1_weight, p_layer1_0_downsample_1_bias, b_layer1_0_downsample_1_running_mean, b_layer1_0_downsample_1_running_var, 0.1, 1e-05);  conv2d_4 = p_layer1_0_downsample_1_weight = p_layer1_0_downsample_1_bias = b_layer1_0_downsample_1_running_mean = b_layer1_0_downsample_1_running_var = None\n",
       "                    getitem_12: \"f32[1, 256, 56, 56]\" = _native_batch_norm_legit_no_training_4[0];  _native_batch_norm_legit_no_training_4 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/resnet.py:160 in forward, code: out += identity\n",
       "                    add: \"f32[1, 256, 56, 56]\" = torch.ops.aten.add.Tensor(getitem_9, getitem_12);  getitem_9 = getitem_12 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_3: \"f32[1, 256, 56, 56]\" = torch.ops.aten.relu.default(add);  add = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_5: \"f32[1, 64, 56, 56]\" = torch.ops.aten.conv2d.default(relu_3, p_layer1_1_conv1_weight);  p_layer1_1_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_5 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_5, p_layer1_1_bn1_weight, p_layer1_1_bn1_bias, b_layer1_1_bn1_running_mean, b_layer1_1_bn1_running_var, 0.1, 1e-05);  conv2d_5 = p_layer1_1_bn1_weight = p_layer1_1_bn1_bias = b_layer1_1_bn1_running_mean = b_layer1_1_bn1_running_var = None\n",
       "                    getitem_15: \"f32[1, 64, 56, 56]\" = _native_batch_norm_legit_no_training_5[0];  _native_batch_norm_legit_no_training_5 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_4: \"f32[1, 64, 56, 56]\" = torch.ops.aten.relu.default(getitem_15);  getitem_15 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_6: \"f32[1, 64, 56, 56]\" = torch.ops.aten.conv2d.default(relu_4, p_layer1_1_conv2_weight, None, [1, 1], [1, 1]);  relu_4 = p_layer1_1_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_6 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_6, p_layer1_1_bn2_weight, p_layer1_1_bn2_bias, b_layer1_1_bn2_running_mean, b_layer1_1_bn2_running_var, 0.1, 1e-05);  conv2d_6 = p_layer1_1_bn2_weight = p_layer1_1_bn2_bias = b_layer1_1_bn2_running_mean = b_layer1_1_bn2_running_var = None\n",
       "                    getitem_18: \"f32[1, 64, 56, 56]\" = _native_batch_norm_legit_no_training_6[0];  _native_batch_norm_legit_no_training_6 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_5: \"f32[1, 64, 56, 56]\" = torch.ops.aten.relu.default(getitem_18);  getitem_18 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_7: \"f32[1, 256, 56, 56]\" = torch.ops.aten.conv2d.default(relu_5, p_layer1_1_conv3_weight);  relu_5 = p_layer1_1_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_7 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_7, p_layer1_1_bn3_weight, p_layer1_1_bn3_bias, b_layer1_1_bn3_running_mean, b_layer1_1_bn3_running_var, 0.1, 1e-05);  conv2d_7 = p_layer1_1_bn3_weight = p_layer1_1_bn3_bias = b_layer1_1_bn3_running_mean = b_layer1_1_bn3_running_var = None\n",
       "                    getitem_21: \"f32[1, 256, 56, 56]\" = _native_batch_norm_legit_no_training_7[0];  _native_batch_norm_legit_no_training_7 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/resnet.py:160 in forward, code: out += identity\n",
       "                    add_1: \"f32[1, 256, 56, 56]\" = torch.ops.aten.add.Tensor(getitem_21, relu_3);  getitem_21 = relu_3 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_6: \"f32[1, 256, 56, 56]\" = torch.ops.aten.relu.default(add_1);  add_1 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_8: \"f32[1, 64, 56, 56]\" = torch.ops.aten.conv2d.default(relu_6, p_layer1_2_conv1_weight);  p_layer1_2_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_8 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_8, p_layer1_2_bn1_weight, p_layer1_2_bn1_bias, b_layer1_2_bn1_running_mean, b_layer1_2_bn1_running_var, 0.1, 1e-05);  conv2d_8 = p_layer1_2_bn1_weight = p_layer1_2_bn1_bias = b_layer1_2_bn1_running_mean = b_layer1_2_bn1_running_var = None\n",
       "                    getitem_24: \"f32[1, 64, 56, 56]\" = _native_batch_norm_legit_no_training_8[0];  _native_batch_norm_legit_no_training_8 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_7: \"f32[1, 64, 56, 56]\" = torch.ops.aten.relu.default(getitem_24);  getitem_24 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_9: \"f32[1, 64, 56, 56]\" = torch.ops.aten.conv2d.default(relu_7, p_layer1_2_conv2_weight, None, [1, 1], [1, 1]);  relu_7 = p_layer1_2_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_9 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_9, p_layer1_2_bn2_weight, p_layer1_2_bn2_bias, b_layer1_2_bn2_running_mean, b_layer1_2_bn2_running_var, 0.1, 1e-05);  conv2d_9 = p_layer1_2_bn2_weight = p_layer1_2_bn2_bias = b_layer1_2_bn2_running_mean = b_layer1_2_bn2_running_var = None\n",
       "                    getitem_27: \"f32[1, 64, 56, 56]\" = _native_batch_norm_legit_no_training_9[0];  _native_batch_norm_legit_no_training_9 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_8: \"f32[1, 64, 56, 56]\" = torch.ops.aten.relu.default(getitem_27);  getitem_27 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_10: \"f32[1, 256, 56, 56]\" = torch.ops.aten.conv2d.default(relu_8, p_layer1_2_conv3_weight);  relu_8 = p_layer1_2_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_10 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_10, p_layer1_2_bn3_weight, p_layer1_2_bn3_bias, b_layer1_2_bn3_running_mean, b_layer1_2_bn3_running_var, 0.1, 1e-05);  conv2d_10 = p_layer1_2_bn3_weight = p_layer1_2_bn3_bias = b_layer1_2_bn3_running_mean = b_layer1_2_bn3_running_var = None\n",
       "                    getitem_30: \"f32[1, 256, 56, 56]\" = _native_batch_norm_legit_no_training_10[0];  _native_batch_norm_legit_no_training_10 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/resnet.py:160 in forward, code: out += identity\n",
       "                    add_2: \"f32[1, 256, 56, 56]\" = torch.ops.aten.add.Tensor(getitem_30, relu_6);  getitem_30 = relu_6 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_9: \"f32[1, 256, 56, 56]\" = torch.ops.aten.relu.default(add_2);  add_2 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_11: \"f32[1, 128, 56, 56]\" = torch.ops.aten.conv2d.default(relu_9, p_layer2_0_conv1_weight);  p_layer2_0_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_11 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_11, p_layer2_0_bn1_weight, p_layer2_0_bn1_bias, b_layer2_0_bn1_running_mean, b_layer2_0_bn1_running_var, 0.1, 1e-05);  conv2d_11 = p_layer2_0_bn1_weight = p_layer2_0_bn1_bias = b_layer2_0_bn1_running_mean = b_layer2_0_bn1_running_var = None\n",
       "                    getitem_33: \"f32[1, 128, 56, 56]\" = _native_batch_norm_legit_no_training_11[0];  _native_batch_norm_legit_no_training_11 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_10: \"f32[1, 128, 56, 56]\" = torch.ops.aten.relu.default(getitem_33);  getitem_33 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_12: \"f32[1, 128, 28, 28]\" = torch.ops.aten.conv2d.default(relu_10, p_layer2_0_conv2_weight, None, [2, 2], [1, 1]);  relu_10 = p_layer2_0_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_12 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_12, p_layer2_0_bn2_weight, p_layer2_0_bn2_bias, b_layer2_0_bn2_running_mean, b_layer2_0_bn2_running_var, 0.1, 1e-05);  conv2d_12 = p_layer2_0_bn2_weight = p_layer2_0_bn2_bias = b_layer2_0_bn2_running_mean = b_layer2_0_bn2_running_var = None\n",
       "                    getitem_36: \"f32[1, 128, 28, 28]\" = _native_batch_norm_legit_no_training_12[0];  _native_batch_norm_legit_no_training_12 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_11: \"f32[1, 128, 28, 28]\" = torch.ops.aten.relu.default(getitem_36);  getitem_36 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_13: \"f32[1, 512, 28, 28]\" = torch.ops.aten.conv2d.default(relu_11, p_layer2_0_conv3_weight);  relu_11 = p_layer2_0_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_13 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_13, p_layer2_0_bn3_weight, p_layer2_0_bn3_bias, b_layer2_0_bn3_running_mean, b_layer2_0_bn3_running_var, 0.1, 1e-05);  conv2d_13 = p_layer2_0_bn3_weight = p_layer2_0_bn3_bias = b_layer2_0_bn3_running_mean = b_layer2_0_bn3_running_var = None\n",
       "                    getitem_39: \"f32[1, 512, 28, 28]\" = _native_batch_norm_legit_no_training_13[0];  _native_batch_norm_legit_no_training_13 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_14: \"f32[1, 512, 28, 28]\" = torch.ops.aten.conv2d.default(relu_9, p_layer2_0_downsample_0_weight, None, [2, 2]);  relu_9 = p_layer2_0_downsample_0_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_14 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_14, p_layer2_0_downsample_1_weight, p_layer2_0_downsample_1_bias, b_layer2_0_downsample_1_running_mean, b_layer2_0_downsample_1_running_var, 0.1, 1e-05);  conv2d_14 = p_layer2_0_downsample_1_weight = p_layer2_0_downsample_1_bias = b_layer2_0_downsample_1_running_mean = b_layer2_0_downsample_1_running_var = None\n",
       "                    getitem_42: \"f32[1, 512, 28, 28]\" = _native_batch_norm_legit_no_training_14[0];  _native_batch_norm_legit_no_training_14 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/resnet.py:160 in forward, code: out += identity\n",
       "                    add_3: \"f32[1, 512, 28, 28]\" = torch.ops.aten.add.Tensor(getitem_39, getitem_42);  getitem_39 = getitem_42 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_12: \"f32[1, 512, 28, 28]\" = torch.ops.aten.relu.default(add_3);  add_3 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_15: \"f32[1, 128, 28, 28]\" = torch.ops.aten.conv2d.default(relu_12, p_layer2_1_conv1_weight);  p_layer2_1_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_15 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_15, p_layer2_1_bn1_weight, p_layer2_1_bn1_bias, b_layer2_1_bn1_running_mean, b_layer2_1_bn1_running_var, 0.1, 1e-05);  conv2d_15 = p_layer2_1_bn1_weight = p_layer2_1_bn1_bias = b_layer2_1_bn1_running_mean = b_layer2_1_bn1_running_var = None\n",
       "                    getitem_45: \"f32[1, 128, 28, 28]\" = _native_batch_norm_legit_no_training_15[0];  _native_batch_norm_legit_no_training_15 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_13: \"f32[1, 128, 28, 28]\" = torch.ops.aten.relu.default(getitem_45);  getitem_45 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_16: \"f32[1, 128, 28, 28]\" = torch.ops.aten.conv2d.default(relu_13, p_layer2_1_conv2_weight, None, [1, 1], [1, 1]);  relu_13 = p_layer2_1_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_16 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_16, p_layer2_1_bn2_weight, p_layer2_1_bn2_bias, b_layer2_1_bn2_running_mean, b_layer2_1_bn2_running_var, 0.1, 1e-05);  conv2d_16 = p_layer2_1_bn2_weight = p_layer2_1_bn2_bias = b_layer2_1_bn2_running_mean = b_layer2_1_bn2_running_var = None\n",
       "                    getitem_48: \"f32[1, 128, 28, 28]\" = _native_batch_norm_legit_no_training_16[0];  _native_batch_norm_legit_no_training_16 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_14: \"f32[1, 128, 28, 28]\" = torch.ops.aten.relu.default(getitem_48);  getitem_48 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_17: \"f32[1, 512, 28, 28]\" = torch.ops.aten.conv2d.default(relu_14, p_layer2_1_conv3_weight);  relu_14 = p_layer2_1_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_17 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_17, p_layer2_1_bn3_weight, p_layer2_1_bn3_bias, b_layer2_1_bn3_running_mean, b_layer2_1_bn3_running_var, 0.1, 1e-05);  conv2d_17 = p_layer2_1_bn3_weight = p_layer2_1_bn3_bias = b_layer2_1_bn3_running_mean = b_layer2_1_bn3_running_var = None\n",
       "                    getitem_51: \"f32[1, 512, 28, 28]\" = _native_batch_norm_legit_no_training_17[0];  _native_batch_norm_legit_no_training_17 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/resnet.py:160 in forward, code: out += identity\n",
       "                    add_4: \"f32[1, 512, 28, 28]\" = torch.ops.aten.add.Tensor(getitem_51, relu_12);  getitem_51 = relu_12 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_15: \"f32[1, 512, 28, 28]\" = torch.ops.aten.relu.default(add_4);  add_4 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_18: \"f32[1, 128, 28, 28]\" = torch.ops.aten.conv2d.default(relu_15, p_layer2_2_conv1_weight);  p_layer2_2_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_18 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_18, p_layer2_2_bn1_weight, p_layer2_2_bn1_bias, b_layer2_2_bn1_running_mean, b_layer2_2_bn1_running_var, 0.1, 1e-05);  conv2d_18 = p_layer2_2_bn1_weight = p_layer2_2_bn1_bias = b_layer2_2_bn1_running_mean = b_layer2_2_bn1_running_var = None\n",
       "                    getitem_54: \"f32[1, 128, 28, 28]\" = _native_batch_norm_legit_no_training_18[0];  _native_batch_norm_legit_no_training_18 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_16: \"f32[1, 128, 28, 28]\" = torch.ops.aten.relu.default(getitem_54);  getitem_54 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_19: \"f32[1, 128, 28, 28]\" = torch.ops.aten.conv2d.default(relu_16, p_layer2_2_conv2_weight, None, [1, 1], [1, 1]);  relu_16 = p_layer2_2_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_19 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_19, p_layer2_2_bn2_weight, p_layer2_2_bn2_bias, b_layer2_2_bn2_running_mean, b_layer2_2_bn2_running_var, 0.1, 1e-05);  conv2d_19 = p_layer2_2_bn2_weight = p_layer2_2_bn2_bias = b_layer2_2_bn2_running_mean = b_layer2_2_bn2_running_var = None\n",
       "                    getitem_57: \"f32[1, 128, 28, 28]\" = _native_batch_norm_legit_no_training_19[0];  _native_batch_norm_legit_no_training_19 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_17: \"f32[1, 128, 28, 28]\" = torch.ops.aten.relu.default(getitem_57);  getitem_57 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_20: \"f32[1, 512, 28, 28]\" = torch.ops.aten.conv2d.default(relu_17, p_layer2_2_conv3_weight);  relu_17 = p_layer2_2_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_20 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_20, p_layer2_2_bn3_weight, p_layer2_2_bn3_bias, b_layer2_2_bn3_running_mean, b_layer2_2_bn3_running_var, 0.1, 1e-05);  conv2d_20 = p_layer2_2_bn3_weight = p_layer2_2_bn3_bias = b_layer2_2_bn3_running_mean = b_layer2_2_bn3_running_var = None\n",
       "                    getitem_60: \"f32[1, 512, 28, 28]\" = _native_batch_norm_legit_no_training_20[0];  _native_batch_norm_legit_no_training_20 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/resnet.py:160 in forward, code: out += identity\n",
       "                    add_5: \"f32[1, 512, 28, 28]\" = torch.ops.aten.add.Tensor(getitem_60, relu_15);  getitem_60 = relu_15 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_18: \"f32[1, 512, 28, 28]\" = torch.ops.aten.relu.default(add_5);  add_5 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_21: \"f32[1, 128, 28, 28]\" = torch.ops.aten.conv2d.default(relu_18, p_layer2_3_conv1_weight);  p_layer2_3_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_21 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_21, p_layer2_3_bn1_weight, p_layer2_3_bn1_bias, b_layer2_3_bn1_running_mean, b_layer2_3_bn1_running_var, 0.1, 1e-05);  conv2d_21 = p_layer2_3_bn1_weight = p_layer2_3_bn1_bias = b_layer2_3_bn1_running_mean = b_layer2_3_bn1_running_var = None\n",
       "                    getitem_63: \"f32[1, 128, 28, 28]\" = _native_batch_norm_legit_no_training_21[0];  _native_batch_norm_legit_no_training_21 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_19: \"f32[1, 128, 28, 28]\" = torch.ops.aten.relu.default(getitem_63);  getitem_63 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_22: \"f32[1, 128, 28, 28]\" = torch.ops.aten.conv2d.default(relu_19, p_layer2_3_conv2_weight, None, [1, 1], [1, 1]);  relu_19 = p_layer2_3_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_22 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_22, p_layer2_3_bn2_weight, p_layer2_3_bn2_bias, b_layer2_3_bn2_running_mean, b_layer2_3_bn2_running_var, 0.1, 1e-05);  conv2d_22 = p_layer2_3_bn2_weight = p_layer2_3_bn2_bias = b_layer2_3_bn2_running_mean = b_layer2_3_bn2_running_var = None\n",
       "                    getitem_66: \"f32[1, 128, 28, 28]\" = _native_batch_norm_legit_no_training_22[0];  _native_batch_norm_legit_no_training_22 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_20: \"f32[1, 128, 28, 28]\" = torch.ops.aten.relu.default(getitem_66);  getitem_66 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_23: \"f32[1, 512, 28, 28]\" = torch.ops.aten.conv2d.default(relu_20, p_layer2_3_conv3_weight);  relu_20 = p_layer2_3_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_23 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_23, p_layer2_3_bn3_weight, p_layer2_3_bn3_bias, b_layer2_3_bn3_running_mean, b_layer2_3_bn3_running_var, 0.1, 1e-05);  conv2d_23 = p_layer2_3_bn3_weight = p_layer2_3_bn3_bias = b_layer2_3_bn3_running_mean = b_layer2_3_bn3_running_var = None\n",
       "                    getitem_69: \"f32[1, 512, 28, 28]\" = _native_batch_norm_legit_no_training_23[0];  _native_batch_norm_legit_no_training_23 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/resnet.py:160 in forward, code: out += identity\n",
       "                    add_6: \"f32[1, 512, 28, 28]\" = torch.ops.aten.add.Tensor(getitem_69, relu_18);  getitem_69 = relu_18 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_21: \"f32[1, 512, 28, 28]\" = torch.ops.aten.relu.default(add_6);  add_6 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_24: \"f32[1, 256, 28, 28]\" = torch.ops.aten.conv2d.default(relu_21, p_layer3_0_conv1_weight);  p_layer3_0_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_24 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_24, p_layer3_0_bn1_weight, p_layer3_0_bn1_bias, b_layer3_0_bn1_running_mean, b_layer3_0_bn1_running_var, 0.1, 1e-05);  conv2d_24 = p_layer3_0_bn1_weight = p_layer3_0_bn1_bias = b_layer3_0_bn1_running_mean = b_layer3_0_bn1_running_var = None\n",
       "                    getitem_72: \"f32[1, 256, 28, 28]\" = _native_batch_norm_legit_no_training_24[0];  _native_batch_norm_legit_no_training_24 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_22: \"f32[1, 256, 28, 28]\" = torch.ops.aten.relu.default(getitem_72);  getitem_72 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_25: \"f32[1, 256, 14, 14]\" = torch.ops.aten.conv2d.default(relu_22, p_layer3_0_conv2_weight, None, [2, 2], [1, 1]);  relu_22 = p_layer3_0_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_25 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_25, p_layer3_0_bn2_weight, p_layer3_0_bn2_bias, b_layer3_0_bn2_running_mean, b_layer3_0_bn2_running_var, 0.1, 1e-05);  conv2d_25 = p_layer3_0_bn2_weight = p_layer3_0_bn2_bias = b_layer3_0_bn2_running_mean = b_layer3_0_bn2_running_var = None\n",
       "                    getitem_75: \"f32[1, 256, 14, 14]\" = _native_batch_norm_legit_no_training_25[0];  _native_batch_norm_legit_no_training_25 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_23: \"f32[1, 256, 14, 14]\" = torch.ops.aten.relu.default(getitem_75);  getitem_75 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_26: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.conv2d.default(relu_23, p_layer3_0_conv3_weight);  relu_23 = p_layer3_0_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_26 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_26, p_layer3_0_bn3_weight, p_layer3_0_bn3_bias, b_layer3_0_bn3_running_mean, b_layer3_0_bn3_running_var, 0.1, 1e-05);  conv2d_26 = p_layer3_0_bn3_weight = p_layer3_0_bn3_bias = b_layer3_0_bn3_running_mean = b_layer3_0_bn3_running_var = None\n",
       "                    getitem_78: \"f32[1, 1024, 14, 14]\" = _native_batch_norm_legit_no_training_26[0];  _native_batch_norm_legit_no_training_26 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_27: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.conv2d.default(relu_21, p_layer3_0_downsample_0_weight, None, [2, 2]);  relu_21 = p_layer3_0_downsample_0_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_27 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_27, p_layer3_0_downsample_1_weight, p_layer3_0_downsample_1_bias, b_layer3_0_downsample_1_running_mean, b_layer3_0_downsample_1_running_var, 0.1, 1e-05);  conv2d_27 = p_layer3_0_downsample_1_weight = p_layer3_0_downsample_1_bias = b_layer3_0_downsample_1_running_mean = b_layer3_0_downsample_1_running_var = None\n",
       "                    getitem_81: \"f32[1, 1024, 14, 14]\" = _native_batch_norm_legit_no_training_27[0];  _native_batch_norm_legit_no_training_27 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/resnet.py:160 in forward, code: out += identity\n",
       "                    add_7: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.add.Tensor(getitem_78, getitem_81);  getitem_78 = getitem_81 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_24: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.relu.default(add_7);  add_7 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_28: \"f32[1, 256, 14, 14]\" = torch.ops.aten.conv2d.default(relu_24, p_layer3_1_conv1_weight);  p_layer3_1_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_28 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_28, p_layer3_1_bn1_weight, p_layer3_1_bn1_bias, b_layer3_1_bn1_running_mean, b_layer3_1_bn1_running_var, 0.1, 1e-05);  conv2d_28 = p_layer3_1_bn1_weight = p_layer3_1_bn1_bias = b_layer3_1_bn1_running_mean = b_layer3_1_bn1_running_var = None\n",
       "                    getitem_84: \"f32[1, 256, 14, 14]\" = _native_batch_norm_legit_no_training_28[0];  _native_batch_norm_legit_no_training_28 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_25: \"f32[1, 256, 14, 14]\" = torch.ops.aten.relu.default(getitem_84);  getitem_84 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_29: \"f32[1, 256, 14, 14]\" = torch.ops.aten.conv2d.default(relu_25, p_layer3_1_conv2_weight, None, [1, 1], [1, 1]);  relu_25 = p_layer3_1_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_29 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_29, p_layer3_1_bn2_weight, p_layer3_1_bn2_bias, b_layer3_1_bn2_running_mean, b_layer3_1_bn2_running_var, 0.1, 1e-05);  conv2d_29 = p_layer3_1_bn2_weight = p_layer3_1_bn2_bias = b_layer3_1_bn2_running_mean = b_layer3_1_bn2_running_var = None\n",
       "                    getitem_87: \"f32[1, 256, 14, 14]\" = _native_batch_norm_legit_no_training_29[0];  _native_batch_norm_legit_no_training_29 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_26: \"f32[1, 256, 14, 14]\" = torch.ops.aten.relu.default(getitem_87);  getitem_87 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_30: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.conv2d.default(relu_26, p_layer3_1_conv3_weight);  relu_26 = p_layer3_1_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_30 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_30, p_layer3_1_bn3_weight, p_layer3_1_bn3_bias, b_layer3_1_bn3_running_mean, b_layer3_1_bn3_running_var, 0.1, 1e-05);  conv2d_30 = p_layer3_1_bn3_weight = p_layer3_1_bn3_bias = b_layer3_1_bn3_running_mean = b_layer3_1_bn3_running_var = None\n",
       "                    getitem_90: \"f32[1, 1024, 14, 14]\" = _native_batch_norm_legit_no_training_30[0];  _native_batch_norm_legit_no_training_30 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/resnet.py:160 in forward, code: out += identity\n",
       "                    add_8: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.add.Tensor(getitem_90, relu_24);  getitem_90 = relu_24 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_27: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.relu.default(add_8);  add_8 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_31: \"f32[1, 256, 14, 14]\" = torch.ops.aten.conv2d.default(relu_27, p_layer3_2_conv1_weight);  p_layer3_2_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_31 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_31, p_layer3_2_bn1_weight, p_layer3_2_bn1_bias, b_layer3_2_bn1_running_mean, b_layer3_2_bn1_running_var, 0.1, 1e-05);  conv2d_31 = p_layer3_2_bn1_weight = p_layer3_2_bn1_bias = b_layer3_2_bn1_running_mean = b_layer3_2_bn1_running_var = None\n",
       "                    getitem_93: \"f32[1, 256, 14, 14]\" = _native_batch_norm_legit_no_training_31[0];  _native_batch_norm_legit_no_training_31 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_28: \"f32[1, 256, 14, 14]\" = torch.ops.aten.relu.default(getitem_93);  getitem_93 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_32: \"f32[1, 256, 14, 14]\" = torch.ops.aten.conv2d.default(relu_28, p_layer3_2_conv2_weight, None, [1, 1], [1, 1]);  relu_28 = p_layer3_2_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_32 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_32, p_layer3_2_bn2_weight, p_layer3_2_bn2_bias, b_layer3_2_bn2_running_mean, b_layer3_2_bn2_running_var, 0.1, 1e-05);  conv2d_32 = p_layer3_2_bn2_weight = p_layer3_2_bn2_bias = b_layer3_2_bn2_running_mean = b_layer3_2_bn2_running_var = None\n",
       "                    getitem_96: \"f32[1, 256, 14, 14]\" = _native_batch_norm_legit_no_training_32[0];  _native_batch_norm_legit_no_training_32 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_29: \"f32[1, 256, 14, 14]\" = torch.ops.aten.relu.default(getitem_96);  getitem_96 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_33: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.conv2d.default(relu_29, p_layer3_2_conv3_weight);  relu_29 = p_layer3_2_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_33 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_33, p_layer3_2_bn3_weight, p_layer3_2_bn3_bias, b_layer3_2_bn3_running_mean, b_layer3_2_bn3_running_var, 0.1, 1e-05);  conv2d_33 = p_layer3_2_bn3_weight = p_layer3_2_bn3_bias = b_layer3_2_bn3_running_mean = b_layer3_2_bn3_running_var = None\n",
       "                    getitem_99: \"f32[1, 1024, 14, 14]\" = _native_batch_norm_legit_no_training_33[0];  _native_batch_norm_legit_no_training_33 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/resnet.py:160 in forward, code: out += identity\n",
       "                    add_9: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.add.Tensor(getitem_99, relu_27);  getitem_99 = relu_27 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_30: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.relu.default(add_9);  add_9 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_34: \"f32[1, 256, 14, 14]\" = torch.ops.aten.conv2d.default(relu_30, p_layer3_3_conv1_weight);  p_layer3_3_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_34 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_34, p_layer3_3_bn1_weight, p_layer3_3_bn1_bias, b_layer3_3_bn1_running_mean, b_layer3_3_bn1_running_var, 0.1, 1e-05);  conv2d_34 = p_layer3_3_bn1_weight = p_layer3_3_bn1_bias = b_layer3_3_bn1_running_mean = b_layer3_3_bn1_running_var = None\n",
       "                    getitem_102: \"f32[1, 256, 14, 14]\" = _native_batch_norm_legit_no_training_34[0];  _native_batch_norm_legit_no_training_34 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_31: \"f32[1, 256, 14, 14]\" = torch.ops.aten.relu.default(getitem_102);  getitem_102 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_35: \"f32[1, 256, 14, 14]\" = torch.ops.aten.conv2d.default(relu_31, p_layer3_3_conv2_weight, None, [1, 1], [1, 1]);  relu_31 = p_layer3_3_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_35 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_35, p_layer3_3_bn2_weight, p_layer3_3_bn2_bias, b_layer3_3_bn2_running_mean, b_layer3_3_bn2_running_var, 0.1, 1e-05);  conv2d_35 = p_layer3_3_bn2_weight = p_layer3_3_bn2_bias = b_layer3_3_bn2_running_mean = b_layer3_3_bn2_running_var = None\n",
       "                    getitem_105: \"f32[1, 256, 14, 14]\" = _native_batch_norm_legit_no_training_35[0];  _native_batch_norm_legit_no_training_35 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_32: \"f32[1, 256, 14, 14]\" = torch.ops.aten.relu.default(getitem_105);  getitem_105 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_36: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.conv2d.default(relu_32, p_layer3_3_conv3_weight);  relu_32 = p_layer3_3_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_36 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_36, p_layer3_3_bn3_weight, p_layer3_3_bn3_bias, b_layer3_3_bn3_running_mean, b_layer3_3_bn3_running_var, 0.1, 1e-05);  conv2d_36 = p_layer3_3_bn3_weight = p_layer3_3_bn3_bias = b_layer3_3_bn3_running_mean = b_layer3_3_bn3_running_var = None\n",
       "                    getitem_108: \"f32[1, 1024, 14, 14]\" = _native_batch_norm_legit_no_training_36[0];  _native_batch_norm_legit_no_training_36 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/resnet.py:160 in forward, code: out += identity\n",
       "                    add_10: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.add.Tensor(getitem_108, relu_30);  getitem_108 = relu_30 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_33: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.relu.default(add_10);  add_10 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_37: \"f32[1, 256, 14, 14]\" = torch.ops.aten.conv2d.default(relu_33, p_layer3_4_conv1_weight);  p_layer3_4_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_37 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_37, p_layer3_4_bn1_weight, p_layer3_4_bn1_bias, b_layer3_4_bn1_running_mean, b_layer3_4_bn1_running_var, 0.1, 1e-05);  conv2d_37 = p_layer3_4_bn1_weight = p_layer3_4_bn1_bias = b_layer3_4_bn1_running_mean = b_layer3_4_bn1_running_var = None\n",
       "                    getitem_111: \"f32[1, 256, 14, 14]\" = _native_batch_norm_legit_no_training_37[0];  _native_batch_norm_legit_no_training_37 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_34: \"f32[1, 256, 14, 14]\" = torch.ops.aten.relu.default(getitem_111);  getitem_111 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_38: \"f32[1, 256, 14, 14]\" = torch.ops.aten.conv2d.default(relu_34, p_layer3_4_conv2_weight, None, [1, 1], [1, 1]);  relu_34 = p_layer3_4_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_38 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_38, p_layer3_4_bn2_weight, p_layer3_4_bn2_bias, b_layer3_4_bn2_running_mean, b_layer3_4_bn2_running_var, 0.1, 1e-05);  conv2d_38 = p_layer3_4_bn2_weight = p_layer3_4_bn2_bias = b_layer3_4_bn2_running_mean = b_layer3_4_bn2_running_var = None\n",
       "                    getitem_114: \"f32[1, 256, 14, 14]\" = _native_batch_norm_legit_no_training_38[0];  _native_batch_norm_legit_no_training_38 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_35: \"f32[1, 256, 14, 14]\" = torch.ops.aten.relu.default(getitem_114);  getitem_114 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_39: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.conv2d.default(relu_35, p_layer3_4_conv3_weight);  relu_35 = p_layer3_4_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_39 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_39, p_layer3_4_bn3_weight, p_layer3_4_bn3_bias, b_layer3_4_bn3_running_mean, b_layer3_4_bn3_running_var, 0.1, 1e-05);  conv2d_39 = p_layer3_4_bn3_weight = p_layer3_4_bn3_bias = b_layer3_4_bn3_running_mean = b_layer3_4_bn3_running_var = None\n",
       "                    getitem_117: \"f32[1, 1024, 14, 14]\" = _native_batch_norm_legit_no_training_39[0];  _native_batch_norm_legit_no_training_39 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/resnet.py:160 in forward, code: out += identity\n",
       "                    add_11: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.add.Tensor(getitem_117, relu_33);  getitem_117 = relu_33 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_36: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.relu.default(add_11);  add_11 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_40: \"f32[1, 256, 14, 14]\" = torch.ops.aten.conv2d.default(relu_36, p_layer3_5_conv1_weight);  p_layer3_5_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_40 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_40, p_layer3_5_bn1_weight, p_layer3_5_bn1_bias, b_layer3_5_bn1_running_mean, b_layer3_5_bn1_running_var, 0.1, 1e-05);  conv2d_40 = p_layer3_5_bn1_weight = p_layer3_5_bn1_bias = b_layer3_5_bn1_running_mean = b_layer3_5_bn1_running_var = None\n",
       "                    getitem_120: \"f32[1, 256, 14, 14]\" = _native_batch_norm_legit_no_training_40[0];  _native_batch_norm_legit_no_training_40 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_37: \"f32[1, 256, 14, 14]\" = torch.ops.aten.relu.default(getitem_120);  getitem_120 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_41: \"f32[1, 256, 14, 14]\" = torch.ops.aten.conv2d.default(relu_37, p_layer3_5_conv2_weight, None, [1, 1], [1, 1]);  relu_37 = p_layer3_5_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_41 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_41, p_layer3_5_bn2_weight, p_layer3_5_bn2_bias, b_layer3_5_bn2_running_mean, b_layer3_5_bn2_running_var, 0.1, 1e-05);  conv2d_41 = p_layer3_5_bn2_weight = p_layer3_5_bn2_bias = b_layer3_5_bn2_running_mean = b_layer3_5_bn2_running_var = None\n",
       "                    getitem_123: \"f32[1, 256, 14, 14]\" = _native_batch_norm_legit_no_training_41[0];  _native_batch_norm_legit_no_training_41 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_38: \"f32[1, 256, 14, 14]\" = torch.ops.aten.relu.default(getitem_123);  getitem_123 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_42: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.conv2d.default(relu_38, p_layer3_5_conv3_weight);  relu_38 = p_layer3_5_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_42 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_42, p_layer3_5_bn3_weight, p_layer3_5_bn3_bias, b_layer3_5_bn3_running_mean, b_layer3_5_bn3_running_var, 0.1, 1e-05);  conv2d_42 = p_layer3_5_bn3_weight = p_layer3_5_bn3_bias = b_layer3_5_bn3_running_mean = b_layer3_5_bn3_running_var = None\n",
       "                    getitem_126: \"f32[1, 1024, 14, 14]\" = _native_batch_norm_legit_no_training_42[0];  _native_batch_norm_legit_no_training_42 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/resnet.py:160 in forward, code: out += identity\n",
       "                    add_12: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.add.Tensor(getitem_126, relu_36);  getitem_126 = relu_36 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_39: \"f32[1, 1024, 14, 14]\" = torch.ops.aten.relu.default(add_12);  add_12 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_43: \"f32[1, 512, 14, 14]\" = torch.ops.aten.conv2d.default(relu_39, p_layer4_0_conv1_weight);  p_layer4_0_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_43 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_43, p_layer4_0_bn1_weight, p_layer4_0_bn1_bias, b_layer4_0_bn1_running_mean, b_layer4_0_bn1_running_var, 0.1, 1e-05);  conv2d_43 = p_layer4_0_bn1_weight = p_layer4_0_bn1_bias = b_layer4_0_bn1_running_mean = b_layer4_0_bn1_running_var = None\n",
       "                    getitem_129: \"f32[1, 512, 14, 14]\" = _native_batch_norm_legit_no_training_43[0];  _native_batch_norm_legit_no_training_43 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_40: \"f32[1, 512, 14, 14]\" = torch.ops.aten.relu.default(getitem_129);  getitem_129 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_44: \"f32[1, 512, 7, 7]\" = torch.ops.aten.conv2d.default(relu_40, p_layer4_0_conv2_weight, None, [2, 2], [1, 1]);  relu_40 = p_layer4_0_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_44 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_44, p_layer4_0_bn2_weight, p_layer4_0_bn2_bias, b_layer4_0_bn2_running_mean, b_layer4_0_bn2_running_var, 0.1, 1e-05);  conv2d_44 = p_layer4_0_bn2_weight = p_layer4_0_bn2_bias = b_layer4_0_bn2_running_mean = b_layer4_0_bn2_running_var = None\n",
       "                    getitem_132: \"f32[1, 512, 7, 7]\" = _native_batch_norm_legit_no_training_44[0];  _native_batch_norm_legit_no_training_44 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_41: \"f32[1, 512, 7, 7]\" = torch.ops.aten.relu.default(getitem_132);  getitem_132 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_45: \"f32[1, 2048, 7, 7]\" = torch.ops.aten.conv2d.default(relu_41, p_layer4_0_conv3_weight);  relu_41 = p_layer4_0_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_45 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_45, p_layer4_0_bn3_weight, p_layer4_0_bn3_bias, b_layer4_0_bn3_running_mean, b_layer4_0_bn3_running_var, 0.1, 1e-05);  conv2d_45 = p_layer4_0_bn3_weight = p_layer4_0_bn3_bias = b_layer4_0_bn3_running_mean = b_layer4_0_bn3_running_var = None\n",
       "                    getitem_135: \"f32[1, 2048, 7, 7]\" = _native_batch_norm_legit_no_training_45[0];  _native_batch_norm_legit_no_training_45 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_46: \"f32[1, 2048, 7, 7]\" = torch.ops.aten.conv2d.default(relu_39, p_layer4_0_downsample_0_weight, None, [2, 2]);  relu_39 = p_layer4_0_downsample_0_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_46 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_46, p_layer4_0_downsample_1_weight, p_layer4_0_downsample_1_bias, b_layer4_0_downsample_1_running_mean, b_layer4_0_downsample_1_running_var, 0.1, 1e-05);  conv2d_46 = p_layer4_0_downsample_1_weight = p_layer4_0_downsample_1_bias = b_layer4_0_downsample_1_running_mean = b_layer4_0_downsample_1_running_var = None\n",
       "                    getitem_138: \"f32[1, 2048, 7, 7]\" = _native_batch_norm_legit_no_training_46[0];  _native_batch_norm_legit_no_training_46 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/resnet.py:160 in forward, code: out += identity\n",
       "                    add_13: \"f32[1, 2048, 7, 7]\" = torch.ops.aten.add.Tensor(getitem_135, getitem_138);  getitem_135 = getitem_138 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_42: \"f32[1, 2048, 7, 7]\" = torch.ops.aten.relu.default(add_13);  add_13 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_47: \"f32[1, 512, 7, 7]\" = torch.ops.aten.conv2d.default(relu_42, p_layer4_1_conv1_weight);  p_layer4_1_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_47 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_47, p_layer4_1_bn1_weight, p_layer4_1_bn1_bias, b_layer4_1_bn1_running_mean, b_layer4_1_bn1_running_var, 0.1, 1e-05);  conv2d_47 = p_layer4_1_bn1_weight = p_layer4_1_bn1_bias = b_layer4_1_bn1_running_mean = b_layer4_1_bn1_running_var = None\n",
       "                    getitem_141: \"f32[1, 512, 7, 7]\" = _native_batch_norm_legit_no_training_47[0];  _native_batch_norm_legit_no_training_47 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_43: \"f32[1, 512, 7, 7]\" = torch.ops.aten.relu.default(getitem_141);  getitem_141 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_48: \"f32[1, 512, 7, 7]\" = torch.ops.aten.conv2d.default(relu_43, p_layer4_1_conv2_weight, None, [1, 1], [1, 1]);  relu_43 = p_layer4_1_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_48 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_48, p_layer4_1_bn2_weight, p_layer4_1_bn2_bias, b_layer4_1_bn2_running_mean, b_layer4_1_bn2_running_var, 0.1, 1e-05);  conv2d_48 = p_layer4_1_bn2_weight = p_layer4_1_bn2_bias = b_layer4_1_bn2_running_mean = b_layer4_1_bn2_running_var = None\n",
       "                    getitem_144: \"f32[1, 512, 7, 7]\" = _native_batch_norm_legit_no_training_48[0];  _native_batch_norm_legit_no_training_48 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_44: \"f32[1, 512, 7, 7]\" = torch.ops.aten.relu.default(getitem_144);  getitem_144 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_49: \"f32[1, 2048, 7, 7]\" = torch.ops.aten.conv2d.default(relu_44, p_layer4_1_conv3_weight);  relu_44 = p_layer4_1_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_49 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_49, p_layer4_1_bn3_weight, p_layer4_1_bn3_bias, b_layer4_1_bn3_running_mean, b_layer4_1_bn3_running_var, 0.1, 1e-05);  conv2d_49 = p_layer4_1_bn3_weight = p_layer4_1_bn3_bias = b_layer4_1_bn3_running_mean = b_layer4_1_bn3_running_var = None\n",
       "                    getitem_147: \"f32[1, 2048, 7, 7]\" = _native_batch_norm_legit_no_training_49[0];  _native_batch_norm_legit_no_training_49 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/resnet.py:160 in forward, code: out += identity\n",
       "                    add_14: \"f32[1, 2048, 7, 7]\" = torch.ops.aten.add.Tensor(getitem_147, relu_42);  getitem_147 = relu_42 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_45: \"f32[1, 2048, 7, 7]\" = torch.ops.aten.relu.default(add_14);  add_14 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_50: \"f32[1, 512, 7, 7]\" = torch.ops.aten.conv2d.default(relu_45, p_layer4_2_conv1_weight);  p_layer4_2_conv1_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_50 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_50, p_layer4_2_bn1_weight, p_layer4_2_bn1_bias, b_layer4_2_bn1_running_mean, b_layer4_2_bn1_running_var, 0.1, 1e-05);  conv2d_50 = p_layer4_2_bn1_weight = p_layer4_2_bn1_bias = b_layer4_2_bn1_running_mean = b_layer4_2_bn1_running_var = None\n",
       "                    getitem_150: \"f32[1, 512, 7, 7]\" = _native_batch_norm_legit_no_training_50[0];  _native_batch_norm_legit_no_training_50 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_46: \"f32[1, 512, 7, 7]\" = torch.ops.aten.relu.default(getitem_150);  getitem_150 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_51: \"f32[1, 512, 7, 7]\" = torch.ops.aten.conv2d.default(relu_46, p_layer4_2_conv2_weight, None, [1, 1], [1, 1]);  relu_46 = p_layer4_2_conv2_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_51 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_51, p_layer4_2_bn2_weight, p_layer4_2_bn2_bias, b_layer4_2_bn2_running_mean, b_layer4_2_bn2_running_var, 0.1, 1e-05);  conv2d_51 = p_layer4_2_bn2_weight = p_layer4_2_bn2_bias = b_layer4_2_bn2_running_mean = b_layer4_2_bn2_running_var = None\n",
       "                    getitem_153: \"f32[1, 512, 7, 7]\" = _native_batch_norm_legit_no_training_51[0];  _native_batch_norm_legit_no_training_51 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_47: \"f32[1, 512, 7, 7]\" = torch.ops.aten.relu.default(getitem_153);  getitem_153 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/conv.py:553 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_52: \"f32[1, 2048, 7, 7]\" = torch.ops.aten.conv2d.default(relu_47, p_layer4_2_conv3_weight);  relu_47 = p_layer4_2_conv3_weight = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:194 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_52 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_52, p_layer4_2_bn3_weight, p_layer4_2_bn3_bias, b_layer4_2_bn3_running_mean, b_layer4_2_bn3_running_var, 0.1, 1e-05);  conv2d_52 = p_layer4_2_bn3_weight = p_layer4_2_bn3_bias = b_layer4_2_bn3_running_mean = b_layer4_2_bn3_running_var = None\n",
       "                    getitem_156: \"f32[1, 2048, 7, 7]\" = _native_batch_norm_legit_no_training_52[0];  _native_batch_norm_legit_no_training_52 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/resnet.py:160 in forward, code: out += identity\n",
       "                    add_15: \"f32[1, 2048, 7, 7]\" = torch.ops.aten.add.Tensor(getitem_156, relu_45);  getitem_156 = relu_45 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/activation.py:143 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_48: \"f32[1, 2048, 7, 7]\" = torch.ops.aten.relu.default(add_15);  add_15 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/pooling.py:1510 in forward, code: return F.adaptive_avg_pool2d(input, self.output_size)\n",
       "                    mean: \"f32[1, 2048, 1, 1]\" = torch.ops.aten.mean.dim(relu_48, [-1, -2], True);  relu_48 = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/resnet.py:285 in forward, code: return self._forward_impl(x)\n",
       "                    view: \"f32[1, 2048]\" = torch.ops.aten.view.default(mean, [1, 2048]);  mean = None\n",
       "            \n",
       "                    # File: /home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear: \"f32[1, 1000]\" = torch.ops.aten.linear.default(view, p_fc_weight, p_fc_bias);  view = p_fc_weight = p_fc_bias = None\n",
       "                    return (linear,)\n",
       "            \n",
       "        Graph signature: \n",
       "            # inputs\n",
       "            p_conv1_weight: PARAMETER target='conv1.weight'\n",
       "            p_bn1_weight: PARAMETER target='bn1.weight'\n",
       "            p_bn1_bias: PARAMETER target='bn1.bias'\n",
       "            p_layer1_0_conv1_weight: PARAMETER target='layer1.0.conv1.weight'\n",
       "            p_layer1_0_bn1_weight: PARAMETER target='layer1.0.bn1.weight'\n",
       "            p_layer1_0_bn1_bias: PARAMETER target='layer1.0.bn1.bias'\n",
       "            p_layer1_0_conv2_weight: PARAMETER target='layer1.0.conv2.weight'\n",
       "            p_layer1_0_bn2_weight: PARAMETER target='layer1.0.bn2.weight'\n",
       "            p_layer1_0_bn2_bias: PARAMETER target='layer1.0.bn2.bias'\n",
       "            p_layer1_0_conv3_weight: PARAMETER target='layer1.0.conv3.weight'\n",
       "            p_layer1_0_bn3_weight: PARAMETER target='layer1.0.bn3.weight'\n",
       "            p_layer1_0_bn3_bias: PARAMETER target='layer1.0.bn3.bias'\n",
       "            p_layer1_0_downsample_0_weight: PARAMETER target='layer1.0.downsample.0.weight'\n",
       "            p_layer1_0_downsample_1_weight: PARAMETER target='layer1.0.downsample.1.weight'\n",
       "            p_layer1_0_downsample_1_bias: PARAMETER target='layer1.0.downsample.1.bias'\n",
       "            p_layer1_1_conv1_weight: PARAMETER target='layer1.1.conv1.weight'\n",
       "            p_layer1_1_bn1_weight: PARAMETER target='layer1.1.bn1.weight'\n",
       "            p_layer1_1_bn1_bias: PARAMETER target='layer1.1.bn1.bias'\n",
       "            p_layer1_1_conv2_weight: PARAMETER target='layer1.1.conv2.weight'\n",
       "            p_layer1_1_bn2_weight: PARAMETER target='layer1.1.bn2.weight'\n",
       "            p_layer1_1_bn2_bias: PARAMETER target='layer1.1.bn2.bias'\n",
       "            p_layer1_1_conv3_weight: PARAMETER target='layer1.1.conv3.weight'\n",
       "            p_layer1_1_bn3_weight: PARAMETER target='layer1.1.bn3.weight'\n",
       "            p_layer1_1_bn3_bias: PARAMETER target='layer1.1.bn3.bias'\n",
       "            p_layer1_2_conv1_weight: PARAMETER target='layer1.2.conv1.weight'\n",
       "            p_layer1_2_bn1_weight: PARAMETER target='layer1.2.bn1.weight'\n",
       "            p_layer1_2_bn1_bias: PARAMETER target='layer1.2.bn1.bias'\n",
       "            p_layer1_2_conv2_weight: PARAMETER target='layer1.2.conv2.weight'\n",
       "            p_layer1_2_bn2_weight: PARAMETER target='layer1.2.bn2.weight'\n",
       "            p_layer1_2_bn2_bias: PARAMETER target='layer1.2.bn2.bias'\n",
       "            p_layer1_2_conv3_weight: PARAMETER target='layer1.2.conv3.weight'\n",
       "            p_layer1_2_bn3_weight: PARAMETER target='layer1.2.bn3.weight'\n",
       "            p_layer1_2_bn3_bias: PARAMETER target='layer1.2.bn3.bias'\n",
       "            p_layer2_0_conv1_weight: PARAMETER target='layer2.0.conv1.weight'\n",
       "            p_layer2_0_bn1_weight: PARAMETER target='layer2.0.bn1.weight'\n",
       "            p_layer2_0_bn1_bias: PARAMETER target='layer2.0.bn1.bias'\n",
       "            p_layer2_0_conv2_weight: PARAMETER target='layer2.0.conv2.weight'\n",
       "            p_layer2_0_bn2_weight: PARAMETER target='layer2.0.bn2.weight'\n",
       "            p_layer2_0_bn2_bias: PARAMETER target='layer2.0.bn2.bias'\n",
       "            p_layer2_0_conv3_weight: PARAMETER target='layer2.0.conv3.weight'\n",
       "            p_layer2_0_bn3_weight: PARAMETER target='layer2.0.bn3.weight'\n",
       "            p_layer2_0_bn3_bias: PARAMETER target='layer2.0.bn3.bias'\n",
       "            p_layer2_0_downsample_0_weight: PARAMETER target='layer2.0.downsample.0.weight'\n",
       "            p_layer2_0_downsample_1_weight: PARAMETER target='layer2.0.downsample.1.weight'\n",
       "            p_layer2_0_downsample_1_bias: PARAMETER target='layer2.0.downsample.1.bias'\n",
       "            p_layer2_1_conv1_weight: PARAMETER target='layer2.1.conv1.weight'\n",
       "            p_layer2_1_bn1_weight: PARAMETER target='layer2.1.bn1.weight'\n",
       "            p_layer2_1_bn1_bias: PARAMETER target='layer2.1.bn1.bias'\n",
       "            p_layer2_1_conv2_weight: PARAMETER target='layer2.1.conv2.weight'\n",
       "            p_layer2_1_bn2_weight: PARAMETER target='layer2.1.bn2.weight'\n",
       "            p_layer2_1_bn2_bias: PARAMETER target='layer2.1.bn2.bias'\n",
       "            p_layer2_1_conv3_weight: PARAMETER target='layer2.1.conv3.weight'\n",
       "            p_layer2_1_bn3_weight: PARAMETER target='layer2.1.bn3.weight'\n",
       "            p_layer2_1_bn3_bias: PARAMETER target='layer2.1.bn3.bias'\n",
       "            p_layer2_2_conv1_weight: PARAMETER target='layer2.2.conv1.weight'\n",
       "            p_layer2_2_bn1_weight: PARAMETER target='layer2.2.bn1.weight'\n",
       "            p_layer2_2_bn1_bias: PARAMETER target='layer2.2.bn1.bias'\n",
       "            p_layer2_2_conv2_weight: PARAMETER target='layer2.2.conv2.weight'\n",
       "            p_layer2_2_bn2_weight: PARAMETER target='layer2.2.bn2.weight'\n",
       "            p_layer2_2_bn2_bias: PARAMETER target='layer2.2.bn2.bias'\n",
       "            p_layer2_2_conv3_weight: PARAMETER target='layer2.2.conv3.weight'\n",
       "            p_layer2_2_bn3_weight: PARAMETER target='layer2.2.bn3.weight'\n",
       "            p_layer2_2_bn3_bias: PARAMETER target='layer2.2.bn3.bias'\n",
       "            p_layer2_3_conv1_weight: PARAMETER target='layer2.3.conv1.weight'\n",
       "            p_layer2_3_bn1_weight: PARAMETER target='layer2.3.bn1.weight'\n",
       "            p_layer2_3_bn1_bias: PARAMETER target='layer2.3.bn1.bias'\n",
       "            p_layer2_3_conv2_weight: PARAMETER target='layer2.3.conv2.weight'\n",
       "            p_layer2_3_bn2_weight: PARAMETER target='layer2.3.bn2.weight'\n",
       "            p_layer2_3_bn2_bias: PARAMETER target='layer2.3.bn2.bias'\n",
       "            p_layer2_3_conv3_weight: PARAMETER target='layer2.3.conv3.weight'\n",
       "            p_layer2_3_bn3_weight: PARAMETER target='layer2.3.bn3.weight'\n",
       "            p_layer2_3_bn3_bias: PARAMETER target='layer2.3.bn3.bias'\n",
       "            p_layer3_0_conv1_weight: PARAMETER target='layer3.0.conv1.weight'\n",
       "            p_layer3_0_bn1_weight: PARAMETER target='layer3.0.bn1.weight'\n",
       "            p_layer3_0_bn1_bias: PARAMETER target='layer3.0.bn1.bias'\n",
       "            p_layer3_0_conv2_weight: PARAMETER target='layer3.0.conv2.weight'\n",
       "            p_layer3_0_bn2_weight: PARAMETER target='layer3.0.bn2.weight'\n",
       "            p_layer3_0_bn2_bias: PARAMETER target='layer3.0.bn2.bias'\n",
       "            p_layer3_0_conv3_weight: PARAMETER target='layer3.0.conv3.weight'\n",
       "            p_layer3_0_bn3_weight: PARAMETER target='layer3.0.bn3.weight'\n",
       "            p_layer3_0_bn3_bias: PARAMETER target='layer3.0.bn3.bias'\n",
       "            p_layer3_0_downsample_0_weight: PARAMETER target='layer3.0.downsample.0.weight'\n",
       "            p_layer3_0_downsample_1_weight: PARAMETER target='layer3.0.downsample.1.weight'\n",
       "            p_layer3_0_downsample_1_bias: PARAMETER target='layer3.0.downsample.1.bias'\n",
       "            p_layer3_1_conv1_weight: PARAMETER target='layer3.1.conv1.weight'\n",
       "            p_layer3_1_bn1_weight: PARAMETER target='layer3.1.bn1.weight'\n",
       "            p_layer3_1_bn1_bias: PARAMETER target='layer3.1.bn1.bias'\n",
       "            p_layer3_1_conv2_weight: PARAMETER target='layer3.1.conv2.weight'\n",
       "            p_layer3_1_bn2_weight: PARAMETER target='layer3.1.bn2.weight'\n",
       "            p_layer3_1_bn2_bias: PARAMETER target='layer3.1.bn2.bias'\n",
       "            p_layer3_1_conv3_weight: PARAMETER target='layer3.1.conv3.weight'\n",
       "            p_layer3_1_bn3_weight: PARAMETER target='layer3.1.bn3.weight'\n",
       "            p_layer3_1_bn3_bias: PARAMETER target='layer3.1.bn3.bias'\n",
       "            p_layer3_2_conv1_weight: PARAMETER target='layer3.2.conv1.weight'\n",
       "            p_layer3_2_bn1_weight: PARAMETER target='layer3.2.bn1.weight'\n",
       "            p_layer3_2_bn1_bias: PARAMETER target='layer3.2.bn1.bias'\n",
       "            p_layer3_2_conv2_weight: PARAMETER target='layer3.2.conv2.weight'\n",
       "            p_layer3_2_bn2_weight: PARAMETER target='layer3.2.bn2.weight'\n",
       "            p_layer3_2_bn2_bias: PARAMETER target='layer3.2.bn2.bias'\n",
       "            p_layer3_2_conv3_weight: PARAMETER target='layer3.2.conv3.weight'\n",
       "            p_layer3_2_bn3_weight: PARAMETER target='layer3.2.bn3.weight'\n",
       "            p_layer3_2_bn3_bias: PARAMETER target='layer3.2.bn3.bias'\n",
       "            p_layer3_3_conv1_weight: PARAMETER target='layer3.3.conv1.weight'\n",
       "            p_layer3_3_bn1_weight: PARAMETER target='layer3.3.bn1.weight'\n",
       "            p_layer3_3_bn1_bias: PARAMETER target='layer3.3.bn1.bias'\n",
       "            p_layer3_3_conv2_weight: PARAMETER target='layer3.3.conv2.weight'\n",
       "            p_layer3_3_bn2_weight: PARAMETER target='layer3.3.bn2.weight'\n",
       "            p_layer3_3_bn2_bias: PARAMETER target='layer3.3.bn2.bias'\n",
       "            p_layer3_3_conv3_weight: PARAMETER target='layer3.3.conv3.weight'\n",
       "            p_layer3_3_bn3_weight: PARAMETER target='layer3.3.bn3.weight'\n",
       "            p_layer3_3_bn3_bias: PARAMETER target='layer3.3.bn3.bias'\n",
       "            p_layer3_4_conv1_weight: PARAMETER target='layer3.4.conv1.weight'\n",
       "            p_layer3_4_bn1_weight: PARAMETER target='layer3.4.bn1.weight'\n",
       "            p_layer3_4_bn1_bias: PARAMETER target='layer3.4.bn1.bias'\n",
       "            p_layer3_4_conv2_weight: PARAMETER target='layer3.4.conv2.weight'\n",
       "            p_layer3_4_bn2_weight: PARAMETER target='layer3.4.bn2.weight'\n",
       "            p_layer3_4_bn2_bias: PARAMETER target='layer3.4.bn2.bias'\n",
       "            p_layer3_4_conv3_weight: PARAMETER target='layer3.4.conv3.weight'\n",
       "            p_layer3_4_bn3_weight: PARAMETER target='layer3.4.bn3.weight'\n",
       "            p_layer3_4_bn3_bias: PARAMETER target='layer3.4.bn3.bias'\n",
       "            p_layer3_5_conv1_weight: PARAMETER target='layer3.5.conv1.weight'\n",
       "            p_layer3_5_bn1_weight: PARAMETER target='layer3.5.bn1.weight'\n",
       "            p_layer3_5_bn1_bias: PARAMETER target='layer3.5.bn1.bias'\n",
       "            p_layer3_5_conv2_weight: PARAMETER target='layer3.5.conv2.weight'\n",
       "            p_layer3_5_bn2_weight: PARAMETER target='layer3.5.bn2.weight'\n",
       "            p_layer3_5_bn2_bias: PARAMETER target='layer3.5.bn2.bias'\n",
       "            p_layer3_5_conv3_weight: PARAMETER target='layer3.5.conv3.weight'\n",
       "            p_layer3_5_bn3_weight: PARAMETER target='layer3.5.bn3.weight'\n",
       "            p_layer3_5_bn3_bias: PARAMETER target='layer3.5.bn3.bias'\n",
       "            p_layer4_0_conv1_weight: PARAMETER target='layer4.0.conv1.weight'\n",
       "            p_layer4_0_bn1_weight: PARAMETER target='layer4.0.bn1.weight'\n",
       "            p_layer4_0_bn1_bias: PARAMETER target='layer4.0.bn1.bias'\n",
       "            p_layer4_0_conv2_weight: PARAMETER target='layer4.0.conv2.weight'\n",
       "            p_layer4_0_bn2_weight: PARAMETER target='layer4.0.bn2.weight'\n",
       "            p_layer4_0_bn2_bias: PARAMETER target='layer4.0.bn2.bias'\n",
       "            p_layer4_0_conv3_weight: PARAMETER target='layer4.0.conv3.weight'\n",
       "            p_layer4_0_bn3_weight: PARAMETER target='layer4.0.bn3.weight'\n",
       "            p_layer4_0_bn3_bias: PARAMETER target='layer4.0.bn3.bias'\n",
       "            p_layer4_0_downsample_0_weight: PARAMETER target='layer4.0.downsample.0.weight'\n",
       "            p_layer4_0_downsample_1_weight: PARAMETER target='layer4.0.downsample.1.weight'\n",
       "            p_layer4_0_downsample_1_bias: PARAMETER target='layer4.0.downsample.1.bias'\n",
       "            p_layer4_1_conv1_weight: PARAMETER target='layer4.1.conv1.weight'\n",
       "            p_layer4_1_bn1_weight: PARAMETER target='layer4.1.bn1.weight'\n",
       "            p_layer4_1_bn1_bias: PARAMETER target='layer4.1.bn1.bias'\n",
       "            p_layer4_1_conv2_weight: PARAMETER target='layer4.1.conv2.weight'\n",
       "            p_layer4_1_bn2_weight: PARAMETER target='layer4.1.bn2.weight'\n",
       "            p_layer4_1_bn2_bias: PARAMETER target='layer4.1.bn2.bias'\n",
       "            p_layer4_1_conv3_weight: PARAMETER target='layer4.1.conv3.weight'\n",
       "            p_layer4_1_bn3_weight: PARAMETER target='layer4.1.bn3.weight'\n",
       "            p_layer4_1_bn3_bias: PARAMETER target='layer4.1.bn3.bias'\n",
       "            p_layer4_2_conv1_weight: PARAMETER target='layer4.2.conv1.weight'\n",
       "            p_layer4_2_bn1_weight: PARAMETER target='layer4.2.bn1.weight'\n",
       "            p_layer4_2_bn1_bias: PARAMETER target='layer4.2.bn1.bias'\n",
       "            p_layer4_2_conv2_weight: PARAMETER target='layer4.2.conv2.weight'\n",
       "            p_layer4_2_bn2_weight: PARAMETER target='layer4.2.bn2.weight'\n",
       "            p_layer4_2_bn2_bias: PARAMETER target='layer4.2.bn2.bias'\n",
       "            p_layer4_2_conv3_weight: PARAMETER target='layer4.2.conv3.weight'\n",
       "            p_layer4_2_bn3_weight: PARAMETER target='layer4.2.bn3.weight'\n",
       "            p_layer4_2_bn3_bias: PARAMETER target='layer4.2.bn3.bias'\n",
       "            p_fc_weight: PARAMETER target='fc.weight'\n",
       "            p_fc_bias: PARAMETER target='fc.bias'\n",
       "            b_bn1_running_mean: BUFFER target='bn1.running_mean' persistent=True\n",
       "            b_bn1_running_var: BUFFER target='bn1.running_var' persistent=True\n",
       "            b_bn1_num_batches_tracked: BUFFER target='bn1.num_batches_tracked' persistent=True\n",
       "            b_layer1_0_bn1_running_mean: BUFFER target='layer1.0.bn1.running_mean' persistent=True\n",
       "            b_layer1_0_bn1_running_var: BUFFER target='layer1.0.bn1.running_var' persistent=True\n",
       "            b_layer1_0_bn1_num_batches_tracked: BUFFER target='layer1.0.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer1_0_bn2_running_mean: BUFFER target='layer1.0.bn2.running_mean' persistent=True\n",
       "            b_layer1_0_bn2_running_var: BUFFER target='layer1.0.bn2.running_var' persistent=True\n",
       "            b_layer1_0_bn2_num_batches_tracked: BUFFER target='layer1.0.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer1_0_bn3_running_mean: BUFFER target='layer1.0.bn3.running_mean' persistent=True\n",
       "            b_layer1_0_bn3_running_var: BUFFER target='layer1.0.bn3.running_var' persistent=True\n",
       "            b_layer1_0_bn3_num_batches_tracked: BUFFER target='layer1.0.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer1_0_downsample_1_running_mean: BUFFER target='layer1.0.downsample.1.running_mean' persistent=True\n",
       "            b_layer1_0_downsample_1_running_var: BUFFER target='layer1.0.downsample.1.running_var' persistent=True\n",
       "            b_layer1_0_downsample_1_num_batches_tracked: BUFFER target='layer1.0.downsample.1.num_batches_tracked' persistent=True\n",
       "            b_layer1_1_bn1_running_mean: BUFFER target='layer1.1.bn1.running_mean' persistent=True\n",
       "            b_layer1_1_bn1_running_var: BUFFER target='layer1.1.bn1.running_var' persistent=True\n",
       "            b_layer1_1_bn1_num_batches_tracked: BUFFER target='layer1.1.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer1_1_bn2_running_mean: BUFFER target='layer1.1.bn2.running_mean' persistent=True\n",
       "            b_layer1_1_bn2_running_var: BUFFER target='layer1.1.bn2.running_var' persistent=True\n",
       "            b_layer1_1_bn2_num_batches_tracked: BUFFER target='layer1.1.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer1_1_bn3_running_mean: BUFFER target='layer1.1.bn3.running_mean' persistent=True\n",
       "            b_layer1_1_bn3_running_var: BUFFER target='layer1.1.bn3.running_var' persistent=True\n",
       "            b_layer1_1_bn3_num_batches_tracked: BUFFER target='layer1.1.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer1_2_bn1_running_mean: BUFFER target='layer1.2.bn1.running_mean' persistent=True\n",
       "            b_layer1_2_bn1_running_var: BUFFER target='layer1.2.bn1.running_var' persistent=True\n",
       "            b_layer1_2_bn1_num_batches_tracked: BUFFER target='layer1.2.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer1_2_bn2_running_mean: BUFFER target='layer1.2.bn2.running_mean' persistent=True\n",
       "            b_layer1_2_bn2_running_var: BUFFER target='layer1.2.bn2.running_var' persistent=True\n",
       "            b_layer1_2_bn2_num_batches_tracked: BUFFER target='layer1.2.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer1_2_bn3_running_mean: BUFFER target='layer1.2.bn3.running_mean' persistent=True\n",
       "            b_layer1_2_bn3_running_var: BUFFER target='layer1.2.bn3.running_var' persistent=True\n",
       "            b_layer1_2_bn3_num_batches_tracked: BUFFER target='layer1.2.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer2_0_bn1_running_mean: BUFFER target='layer2.0.bn1.running_mean' persistent=True\n",
       "            b_layer2_0_bn1_running_var: BUFFER target='layer2.0.bn1.running_var' persistent=True\n",
       "            b_layer2_0_bn1_num_batches_tracked: BUFFER target='layer2.0.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer2_0_bn2_running_mean: BUFFER target='layer2.0.bn2.running_mean' persistent=True\n",
       "            b_layer2_0_bn2_running_var: BUFFER target='layer2.0.bn2.running_var' persistent=True\n",
       "            b_layer2_0_bn2_num_batches_tracked: BUFFER target='layer2.0.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer2_0_bn3_running_mean: BUFFER target='layer2.0.bn3.running_mean' persistent=True\n",
       "            b_layer2_0_bn3_running_var: BUFFER target='layer2.0.bn3.running_var' persistent=True\n",
       "            b_layer2_0_bn3_num_batches_tracked: BUFFER target='layer2.0.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer2_0_downsample_1_running_mean: BUFFER target='layer2.0.downsample.1.running_mean' persistent=True\n",
       "            b_layer2_0_downsample_1_running_var: BUFFER target='layer2.0.downsample.1.running_var' persistent=True\n",
       "            b_layer2_0_downsample_1_num_batches_tracked: BUFFER target='layer2.0.downsample.1.num_batches_tracked' persistent=True\n",
       "            b_layer2_1_bn1_running_mean: BUFFER target='layer2.1.bn1.running_mean' persistent=True\n",
       "            b_layer2_1_bn1_running_var: BUFFER target='layer2.1.bn1.running_var' persistent=True\n",
       "            b_layer2_1_bn1_num_batches_tracked: BUFFER target='layer2.1.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer2_1_bn2_running_mean: BUFFER target='layer2.1.bn2.running_mean' persistent=True\n",
       "            b_layer2_1_bn2_running_var: BUFFER target='layer2.1.bn2.running_var' persistent=True\n",
       "            b_layer2_1_bn2_num_batches_tracked: BUFFER target='layer2.1.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer2_1_bn3_running_mean: BUFFER target='layer2.1.bn3.running_mean' persistent=True\n",
       "            b_layer2_1_bn3_running_var: BUFFER target='layer2.1.bn3.running_var' persistent=True\n",
       "            b_layer2_1_bn3_num_batches_tracked: BUFFER target='layer2.1.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer2_2_bn1_running_mean: BUFFER target='layer2.2.bn1.running_mean' persistent=True\n",
       "            b_layer2_2_bn1_running_var: BUFFER target='layer2.2.bn1.running_var' persistent=True\n",
       "            b_layer2_2_bn1_num_batches_tracked: BUFFER target='layer2.2.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer2_2_bn2_running_mean: BUFFER target='layer2.2.bn2.running_mean' persistent=True\n",
       "            b_layer2_2_bn2_running_var: BUFFER target='layer2.2.bn2.running_var' persistent=True\n",
       "            b_layer2_2_bn2_num_batches_tracked: BUFFER target='layer2.2.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer2_2_bn3_running_mean: BUFFER target='layer2.2.bn3.running_mean' persistent=True\n",
       "            b_layer2_2_bn3_running_var: BUFFER target='layer2.2.bn3.running_var' persistent=True\n",
       "            b_layer2_2_bn3_num_batches_tracked: BUFFER target='layer2.2.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer2_3_bn1_running_mean: BUFFER target='layer2.3.bn1.running_mean' persistent=True\n",
       "            b_layer2_3_bn1_running_var: BUFFER target='layer2.3.bn1.running_var' persistent=True\n",
       "            b_layer2_3_bn1_num_batches_tracked: BUFFER target='layer2.3.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer2_3_bn2_running_mean: BUFFER target='layer2.3.bn2.running_mean' persistent=True\n",
       "            b_layer2_3_bn2_running_var: BUFFER target='layer2.3.bn2.running_var' persistent=True\n",
       "            b_layer2_3_bn2_num_batches_tracked: BUFFER target='layer2.3.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer2_3_bn3_running_mean: BUFFER target='layer2.3.bn3.running_mean' persistent=True\n",
       "            b_layer2_3_bn3_running_var: BUFFER target='layer2.3.bn3.running_var' persistent=True\n",
       "            b_layer2_3_bn3_num_batches_tracked: BUFFER target='layer2.3.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer3_0_bn1_running_mean: BUFFER target='layer3.0.bn1.running_mean' persistent=True\n",
       "            b_layer3_0_bn1_running_var: BUFFER target='layer3.0.bn1.running_var' persistent=True\n",
       "            b_layer3_0_bn1_num_batches_tracked: BUFFER target='layer3.0.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer3_0_bn2_running_mean: BUFFER target='layer3.0.bn2.running_mean' persistent=True\n",
       "            b_layer3_0_bn2_running_var: BUFFER target='layer3.0.bn2.running_var' persistent=True\n",
       "            b_layer3_0_bn2_num_batches_tracked: BUFFER target='layer3.0.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer3_0_bn3_running_mean: BUFFER target='layer3.0.bn3.running_mean' persistent=True\n",
       "            b_layer3_0_bn3_running_var: BUFFER target='layer3.0.bn3.running_var' persistent=True\n",
       "            b_layer3_0_bn3_num_batches_tracked: BUFFER target='layer3.0.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer3_0_downsample_1_running_mean: BUFFER target='layer3.0.downsample.1.running_mean' persistent=True\n",
       "            b_layer3_0_downsample_1_running_var: BUFFER target='layer3.0.downsample.1.running_var' persistent=True\n",
       "            b_layer3_0_downsample_1_num_batches_tracked: BUFFER target='layer3.0.downsample.1.num_batches_tracked' persistent=True\n",
       "            b_layer3_1_bn1_running_mean: BUFFER target='layer3.1.bn1.running_mean' persistent=True\n",
       "            b_layer3_1_bn1_running_var: BUFFER target='layer3.1.bn1.running_var' persistent=True\n",
       "            b_layer3_1_bn1_num_batches_tracked: BUFFER target='layer3.1.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer3_1_bn2_running_mean: BUFFER target='layer3.1.bn2.running_mean' persistent=True\n",
       "            b_layer3_1_bn2_running_var: BUFFER target='layer3.1.bn2.running_var' persistent=True\n",
       "            b_layer3_1_bn2_num_batches_tracked: BUFFER target='layer3.1.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer3_1_bn3_running_mean: BUFFER target='layer3.1.bn3.running_mean' persistent=True\n",
       "            b_layer3_1_bn3_running_var: BUFFER target='layer3.1.bn3.running_var' persistent=True\n",
       "            b_layer3_1_bn3_num_batches_tracked: BUFFER target='layer3.1.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer3_2_bn1_running_mean: BUFFER target='layer3.2.bn1.running_mean' persistent=True\n",
       "            b_layer3_2_bn1_running_var: BUFFER target='layer3.2.bn1.running_var' persistent=True\n",
       "            b_layer3_2_bn1_num_batches_tracked: BUFFER target='layer3.2.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer3_2_bn2_running_mean: BUFFER target='layer3.2.bn2.running_mean' persistent=True\n",
       "            b_layer3_2_bn2_running_var: BUFFER target='layer3.2.bn2.running_var' persistent=True\n",
       "            b_layer3_2_bn2_num_batches_tracked: BUFFER target='layer3.2.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer3_2_bn3_running_mean: BUFFER target='layer3.2.bn3.running_mean' persistent=True\n",
       "            b_layer3_2_bn3_running_var: BUFFER target='layer3.2.bn3.running_var' persistent=True\n",
       "            b_layer3_2_bn3_num_batches_tracked: BUFFER target='layer3.2.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer3_3_bn1_running_mean: BUFFER target='layer3.3.bn1.running_mean' persistent=True\n",
       "            b_layer3_3_bn1_running_var: BUFFER target='layer3.3.bn1.running_var' persistent=True\n",
       "            b_layer3_3_bn1_num_batches_tracked: BUFFER target='layer3.3.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer3_3_bn2_running_mean: BUFFER target='layer3.3.bn2.running_mean' persistent=True\n",
       "            b_layer3_3_bn2_running_var: BUFFER target='layer3.3.bn2.running_var' persistent=True\n",
       "            b_layer3_3_bn2_num_batches_tracked: BUFFER target='layer3.3.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer3_3_bn3_running_mean: BUFFER target='layer3.3.bn3.running_mean' persistent=True\n",
       "            b_layer3_3_bn3_running_var: BUFFER target='layer3.3.bn3.running_var' persistent=True\n",
       "            b_layer3_3_bn3_num_batches_tracked: BUFFER target='layer3.3.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer3_4_bn1_running_mean: BUFFER target='layer3.4.bn1.running_mean' persistent=True\n",
       "            b_layer3_4_bn1_running_var: BUFFER target='layer3.4.bn1.running_var' persistent=True\n",
       "            b_layer3_4_bn1_num_batches_tracked: BUFFER target='layer3.4.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer3_4_bn2_running_mean: BUFFER target='layer3.4.bn2.running_mean' persistent=True\n",
       "            b_layer3_4_bn2_running_var: BUFFER target='layer3.4.bn2.running_var' persistent=True\n",
       "            b_layer3_4_bn2_num_batches_tracked: BUFFER target='layer3.4.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer3_4_bn3_running_mean: BUFFER target='layer3.4.bn3.running_mean' persistent=True\n",
       "            b_layer3_4_bn3_running_var: BUFFER target='layer3.4.bn3.running_var' persistent=True\n",
       "            b_layer3_4_bn3_num_batches_tracked: BUFFER target='layer3.4.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer3_5_bn1_running_mean: BUFFER target='layer3.5.bn1.running_mean' persistent=True\n",
       "            b_layer3_5_bn1_running_var: BUFFER target='layer3.5.bn1.running_var' persistent=True\n",
       "            b_layer3_5_bn1_num_batches_tracked: BUFFER target='layer3.5.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer3_5_bn2_running_mean: BUFFER target='layer3.5.bn2.running_mean' persistent=True\n",
       "            b_layer3_5_bn2_running_var: BUFFER target='layer3.5.bn2.running_var' persistent=True\n",
       "            b_layer3_5_bn2_num_batches_tracked: BUFFER target='layer3.5.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer3_5_bn3_running_mean: BUFFER target='layer3.5.bn3.running_mean' persistent=True\n",
       "            b_layer3_5_bn3_running_var: BUFFER target='layer3.5.bn3.running_var' persistent=True\n",
       "            b_layer3_5_bn3_num_batches_tracked: BUFFER target='layer3.5.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer4_0_bn1_running_mean: BUFFER target='layer4.0.bn1.running_mean' persistent=True\n",
       "            b_layer4_0_bn1_running_var: BUFFER target='layer4.0.bn1.running_var' persistent=True\n",
       "            b_layer4_0_bn1_num_batches_tracked: BUFFER target='layer4.0.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer4_0_bn2_running_mean: BUFFER target='layer4.0.bn2.running_mean' persistent=True\n",
       "            b_layer4_0_bn2_running_var: BUFFER target='layer4.0.bn2.running_var' persistent=True\n",
       "            b_layer4_0_bn2_num_batches_tracked: BUFFER target='layer4.0.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer4_0_bn3_running_mean: BUFFER target='layer4.0.bn3.running_mean' persistent=True\n",
       "            b_layer4_0_bn3_running_var: BUFFER target='layer4.0.bn3.running_var' persistent=True\n",
       "            b_layer4_0_bn3_num_batches_tracked: BUFFER target='layer4.0.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer4_0_downsample_1_running_mean: BUFFER target='layer4.0.downsample.1.running_mean' persistent=True\n",
       "            b_layer4_0_downsample_1_running_var: BUFFER target='layer4.0.downsample.1.running_var' persistent=True\n",
       "            b_layer4_0_downsample_1_num_batches_tracked: BUFFER target='layer4.0.downsample.1.num_batches_tracked' persistent=True\n",
       "            b_layer4_1_bn1_running_mean: BUFFER target='layer4.1.bn1.running_mean' persistent=True\n",
       "            b_layer4_1_bn1_running_var: BUFFER target='layer4.1.bn1.running_var' persistent=True\n",
       "            b_layer4_1_bn1_num_batches_tracked: BUFFER target='layer4.1.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer4_1_bn2_running_mean: BUFFER target='layer4.1.bn2.running_mean' persistent=True\n",
       "            b_layer4_1_bn2_running_var: BUFFER target='layer4.1.bn2.running_var' persistent=True\n",
       "            b_layer4_1_bn2_num_batches_tracked: BUFFER target='layer4.1.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer4_1_bn3_running_mean: BUFFER target='layer4.1.bn3.running_mean' persistent=True\n",
       "            b_layer4_1_bn3_running_var: BUFFER target='layer4.1.bn3.running_var' persistent=True\n",
       "            b_layer4_1_bn3_num_batches_tracked: BUFFER target='layer4.1.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer4_2_bn1_running_mean: BUFFER target='layer4.2.bn1.running_mean' persistent=True\n",
       "            b_layer4_2_bn1_running_var: BUFFER target='layer4.2.bn1.running_var' persistent=True\n",
       "            b_layer4_2_bn1_num_batches_tracked: BUFFER target='layer4.2.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer4_2_bn2_running_mean: BUFFER target='layer4.2.bn2.running_mean' persistent=True\n",
       "            b_layer4_2_bn2_running_var: BUFFER target='layer4.2.bn2.running_var' persistent=True\n",
       "            b_layer4_2_bn2_num_batches_tracked: BUFFER target='layer4.2.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer4_2_bn3_running_mean: BUFFER target='layer4.2.bn3.running_mean' persistent=True\n",
       "            b_layer4_2_bn3_running_var: BUFFER target='layer4.2.bn3.running_var' persistent=True\n",
       "            b_layer4_2_bn3_num_batches_tracked: BUFFER target='layer4.2.bn3.num_batches_tracked' persistent=True\n",
       "            x: USER_INPUT\n",
       "    \n",
       "            # outputs\n",
       "            linear: USER_OUTPUT\n",
       "    \n",
       "        Range constraints: {}\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE)\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    ONNX_PATH,\n",
    "    opset_version=OPSET,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes=None, # Optional: specify dynamic axes if needed\n",
    "    external_data=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b2056",
   "metadata": {},
   "source": [
    "### Evaluate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be90581",
   "metadata": {},
   "source": [
    "Đánh giá trên các EP khác nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b035ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "SAMPLES = 1000\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b619048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(session):\n",
    "    print(f\"Evaluating ONNX model with providers: {session._providers}\")\n",
    "    inference_times = []\n",
    "    for _ in tqdm(range(SAMPLES), desc=f\"Running ONNX Inference: \"):\n",
    "        input_data = np.random.randn(1, 3, IMG_SIZE, IMG_SIZE).astype(np.float32)\n",
    "        start_time = time.perf_counter()\n",
    "        __ = session.run(['output'], {'input': input_data})\n",
    "        inference_times.append((time.perf_counter() - start_time) * 1e3) # Convert to milliseconds\n",
    "    # Calculate statistics\n",
    "    mean_time = np.mean(inference_times)\n",
    "    std_time = np.std(inference_times)\n",
    "    min_time = np.min(inference_times)\n",
    "    max_time = np.max(inference_times)\n",
    "    median_time = np.median(inference_times)\n",
    "    p95_time = np.percentile(inference_times, 95)\n",
    "    p99_time = np.percentile(inference_times, 99)\n",
    "\n",
    "    print(f\"Inference Time Statistics (milliseconds):\")\n",
    "    print(f\"  Mean: {mean_time:.6f}\")\n",
    "    print(f\"  Std: {std_time:.6f}\")\n",
    "    print(f\"  Min: {min_time:.6f}\")\n",
    "    print(f\"  Max: {max_time:.6f}\")\n",
    "    print(f\"  Median: {median_time:.6f}\")\n",
    "    print(f\"  95th percentile: {p95_time:.6f}\")\n",
    "    print(f\"  99th percentile: {p99_time:.6f}\")\n",
    "    print(f\"\\nThroughput: {SAMPLES / (sum(inference_times) / 1e3):.2f} samples/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2eddac",
   "metadata": {},
   "source": [
    "#### CPUExcutionProvider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "663d9264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ONNX model with providers: ['CPUExecutionProvider']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a838479483a240b69396742f920f2fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running ONNX Inference:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time Statistics (milliseconds):\n",
      "  Mean: 19.507692\n",
      "  Std: 2.172896\n",
      "  Min: 18.600732\n",
      "  Max: 51.288857\n",
      "  Median: 19.110651\n",
      "  95th percentile: 20.653843\n",
      "  99th percentile: 29.535244\n",
      "\n",
      "Throughput: 51.26 samples/sec\n"
     ]
    }
   ],
   "source": [
    "session_options = ort.SessionOptions()\n",
    "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "session_options.intra_op_num_threads = 4  # giá trị mặc định là 0 (tự động chọn số luồng dựa trên số lõi CPU)\n",
    "session = ort.InferenceSession(ONNX_PATH, sess_options=session_options, providers=['CPUExecutionProvider'])\n",
    "eval_fn(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4df6e",
   "metadata": {},
   "source": [
    "#### CUDAExecutionProvider:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a7eb5",
   "metadata": {},
   "source": [
    "Nếu thiếu cudnn, cài đặt bằng: `conda install cudnn=9 cuda=12 -c nvidia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afb645a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ONNX model with providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5cfcc95f055405fbd910d8d52a51610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running ONNX Inference:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time Statistics (milliseconds):\n",
      "  Mean: 1.504121\n",
      "  Std: 2.448048\n",
      "  Min: 1.380869\n",
      "  Max: 78.874833\n",
      "  Median: 1.435211\n",
      "  95th percentile: 1.463560\n",
      "  99th percentile: 1.487714\n",
      "\n",
      "Throughput: 664.84 samples/sec\n"
     ]
    }
   ],
   "source": [
    "session_options = ort.SessionOptions()\n",
    "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "session = ort.InferenceSession(\n",
    "    ONNX_PATH, \n",
    "    sess_options=session_options, \n",
    "    providers = [\n",
    "        (\"CUDAExecutionProvider\", {\n",
    "            \"device_id\": 0,\n",
    "            \"arena_extend_strategy\": \"kNextPowerOfTwo\",\n",
    "            \"cudnn_conv_algo_search\": \"EXHAUSTIVE\",\n",
    "            \"enable_cuda_graph\": True, # Enable CUDA Graphs for better performance on repeated workloads\n",
    "            \"gpu_mem_limit\": 4 * 1024 * 1024 * 1024,\n",
    "        }),\n",
    "    ]\n",
    ")\n",
    "eval_fn(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb873e5",
   "metadata": {},
   "source": [
    "#### TensorRT:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bf0a41",
   "metadata": {},
   "source": [
    "Cần cài TensorRT độc lập, không có quyền sudo để cài."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea9e38",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'dl (Python 3.10.19)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n dl ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "session_options = ort.SessionOptions()\n",
    "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "session = ort.InferenceSession(\n",
    "    ONNX_PATH, \n",
    "    sess_options=session_options, \n",
    "    providers = ['TensorRTExecutionProvider', 'CUDAExecutionProvider']\n",
    ")\n",
    "eval_fn(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e055cc21",
   "metadata": {},
   "source": [
    "TensorRT:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8d03e",
   "metadata": {},
   "source": [
    "ONNX model -> Convert to TensorRT engine -> inference on TensorRT Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f80698d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/12/2026-16:01:18] [TRT] [W] Unable to determine GPU memory usage: In getGpuMemStatsInBytes at /_src/common/extended/resources.cpp:1180\n",
      "[02/12/2026-16:01:18] [TRT] [E] createInferBuilder: Error Code 6: API Usage Error (CUDA initialization failure with error: 35. Please check your CUDA installation: http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html In operator() at /_src/optimizer/api/builder.cpp:1399)\n",
      "[02/12/2026-16:01:18] [TRT] [E] [checkMacros.cpp::catchCudaError::229] Error Code 1: Cuda Runtime (In catchCudaError at /_src/common/dispatch/checkMacros.cpp:229)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pybind11::init(): factory function returned nullptr",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m\n\u001b[1;32m     42\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(serialized_engine)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Engine saved:\u001b[39m\u001b[38;5;124m\"\u001b[39m, engine_path)\n\u001b[0;32m---> 46\u001b[0m \u001b[43mbuild_engine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet50.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet50.engine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkspace_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\n\u001b[1;32m     51\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m, in \u001b[0;36mbuild_engine\u001b[0;34m(onnx_path, engine_path, fp16, int8, workspace_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_engine\u001b[39m(\n\u001b[1;32m      7\u001b[0m     onnx_path,\n\u001b[1;32m      8\u001b[0m     engine_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     workspace_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m<<\u001b[39m \u001b[38;5;241m30\u001b[39m  \u001b[38;5;66;03m# 4GB\u001b[39;00m\n\u001b[1;32m     12\u001b[0m ):\n\u001b[0;32m---> 13\u001b[0m     builder \u001b[38;5;241m=\u001b[39m \u001b[43mtrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRT_LOGGER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     network \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mcreate_network(\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<<\u001b[39m \u001b[38;5;28mint\u001b[39m(trt\u001b[38;5;241m.\u001b[39mNetworkDefinitionCreationFlag\u001b[38;5;241m.\u001b[39mEXPLICIT_BATCH)\n\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     17\u001b[0m     parser \u001b[38;5;241m=\u001b[39m trt\u001b[38;5;241m.\u001b[39mOnnxParser(network, TRT_LOGGER)\n",
      "\u001b[0;31mTypeError\u001b[0m: pybind11::init(): factory function returned nullptr"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import os\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "\n",
    "def build_engine(\n",
    "    onnx_path,\n",
    "    engine_path,\n",
    "    fp16=True,\n",
    "    int8=False,\n",
    "    workspace_size=4 << 30  # 4GB\n",
    "):\n",
    "    builder = trt.Builder(TRT_LOGGER)\n",
    "    network = builder.create_network(\n",
    "        1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "    )\n",
    "    parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "\n",
    "    config = builder.create_builder_config()\n",
    "    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace_size)\n",
    "\n",
    "    if fp16:\n",
    "        config.set_flag(trt.BuilderFlag.FP16)\n",
    "\n",
    "    if int8:\n",
    "        config.set_flag(trt.BuilderFlag.INT8)\n",
    "        # cần calibrator nếu INT8\n",
    "\n",
    "    # Parse ONNX\n",
    "    with open(onnx_path, \"rb\") as f:\n",
    "        if not parser.parse(f.read()):\n",
    "            for error in range(parser.num_errors):\n",
    "                print(parser.get_error(error))\n",
    "            raise RuntimeError(\"Failed to parse ONNX\")\n",
    "\n",
    "    # Build engine\n",
    "    serialized_engine = builder.build_serialized_network(network, config)\n",
    "    if serialized_engine is None:\n",
    "        raise RuntimeError(\"Engine build failed\")\n",
    "\n",
    "    with open(engine_path, \"wb\") as f:\n",
    "        f.write(serialized_engine)\n",
    "\n",
    "    print(\"✅ Engine saved:\", engine_path)\n",
    "\n",
    "build_engine(\n",
    "    \"resnet50.onnx\",\n",
    "    \"resnet50.engine\",\n",
    "    fp16=True,\n",
    "    workspace_size=4 << 30\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
