{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd98080",
   "metadata": {},
   "source": [
    "### Export ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd98df",
   "metadata": {},
   "source": [
    "ONNX tạo export đồ thị độc lập với phần cứng, tức là dù export trên CPU hay GPU thì graph được tạo ra cũng không đổi. ONNXRuntime mới tối ưu phụ thuộc vào phần cứng, đây là nơi tạo ra sự khác biệt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e5cb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "model = resnet50(pretrained=True)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8830851",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONNX_PATH = \"resnet50.onnx\"\n",
    "IMG_SIZE = 224\n",
    "OPSET = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d57777b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5060/173835326.py:2: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE)\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    ONNX_PATH,\n",
    "    opset_version=OPSET,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes=None # Optional: specify dynamic axes if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b2056",
   "metadata": {},
   "source": [
    "### Evaluate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be90581",
   "metadata": {},
   "source": [
    "Đánh giá trên các EP khác nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b035ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "SAMPLES = 1000\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b619048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(session):\n",
    "    print(f\"Evaluating ONNX model with providers: {session._providers}\")\n",
    "    inference_times = []\n",
    "    for _ in tqdm(range(SAMPLES), desc=f\"Running ONNX Inference: \"):\n",
    "        input_data = np.random.randn(1, 3, IMG_SIZE, IMG_SIZE).astype(np.float32)\n",
    "        start_time = time.perf_counter()\n",
    "        __ = session.run(['output'], {'input': input_data})\n",
    "        inference_times.append((time.perf_counter() - start_time) * 1e3) # Convert to milliseconds\n",
    "    # Calculate statistics\n",
    "    mean_time = np.mean(inference_times)\n",
    "    std_time = np.std(inference_times)\n",
    "    min_time = np.min(inference_times)\n",
    "    max_time = np.max(inference_times)\n",
    "    median_time = np.median(inference_times)\n",
    "    p95_time = np.percentile(inference_times, 95)\n",
    "    p99_time = np.percentile(inference_times, 99)\n",
    "\n",
    "    print(f\"Inference Time Statistics (milliseconds):\")\n",
    "    print(f\"  Mean: {mean_time:.6f}\")\n",
    "    print(f\"  Std: {std_time:.6f}\")\n",
    "    print(f\"  Min: {min_time:.6f}\")\n",
    "    print(f\"  Max: {max_time:.6f}\")\n",
    "    print(f\"  Median: {median_time:.6f}\")\n",
    "    print(f\"  95th percentile: {p95_time:.6f}\")\n",
    "    print(f\"  99th percentile: {p99_time:.6f}\")\n",
    "    print(f\"\\nThroughput: {SAMPLES / (sum(inference_times) / 1e3):.2f} samples/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2eddac",
   "metadata": {},
   "source": [
    "#### CPUExcutionProvider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "663d9264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ONNX model with providers: ['CPUExecutionProvider']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e0879cd6914d3bb06248f60361d877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running ONNX Inference:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time Statistics (milliseconds):\n",
      "  Mean: 21.561758\n",
      "  Std: 6.173452\n",
      "  Min: 18.684868\n",
      "  Max: 95.983619\n",
      "  Median: 19.552237\n",
      "  95th percentile: 33.455162\n",
      "  99th percentile: 49.699594\n",
      "\n",
      "Throughput: 46.38 samples/sec\n"
     ]
    }
   ],
   "source": [
    "session_options = ort.SessionOptions()\n",
    "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "session_options.intra_op_num_threads = 4  # giá trị mặc định là 0 (tự động chọn số luồng dựa trên số lõi CPU)\n",
    "session = ort.InferenceSession(ONNX_PATH, sess_options=session_options, providers=['CPUExecutionProvider'])\n",
    "eval_fn(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4df6e",
   "metadata": {},
   "source": [
    "#### CUDAExecutionProvider:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a7eb5",
   "metadata": {},
   "source": [
    "Nếu thiếu cudnn, cài đặt bằng: `conda install cudnn=9 cuda=12 -c nvidia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb645a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ONNX model with providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aac635d83b744859651f39f39642eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running ONNX Inference:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time Statistics (milliseconds):\n",
      "  Mean: 1.543070\n",
      "  Std: 3.997516\n",
      "  Min: 1.368160\n",
      "  Max: 127.825740\n",
      "  Median: 1.419127\n",
      "  95th percentile: 1.453051\n",
      "  99th percentile: 1.473385\n",
      "\n",
      "Throughput: 648.06 samples/sec\n"
     ]
    }
   ],
   "source": [
    "session_options = ort.SessionOptions()\n",
    "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "session = ort.InferenceSession(\n",
    "    ONNX_PATH, \n",
    "    sess_options=session_options, \n",
    "    providers = [\n",
    "        (\"CUDAExecutionProvider\", {\n",
    "            \"device_id\": 0,\n",
    "            \"arena_extend_strategy\": \"kNextPowerOfTwo\",\n",
    "            \"cudnn_conv_algo_search\": \"EXHAUSTIVE\",\n",
    "            \"enable_cuda_graph\": True, # Enable CUDA Graphs for better performance on repeated workloads\n",
    "            \"gpu_mem_limit\": 4 * 1024 * 1024 * 1024,\n",
    "        }),\n",
    "    ]\n",
    ")\n",
    "eval_fn(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb873e5",
   "metadata": {},
   "source": [
    "#### TensorRT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ea9e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmits/miniconda3/envs/dl/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:123: UserWarning: Specified provider 'TensorRTExecutionProvider' is not in available provider names.Available providers: 'TensorrtExecutionProvider, CUDAExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** EP Error ***************\n",
      "EP Error Unknown Provider Type: TensorRTExecutionProvider when using ['TensorRTExecutionProvider']\n",
      "Falling back to ['CPUExecutionProvider'] and retrying.\n",
      "****************************************\n",
      "Evaluating ONNX model with providers: ['CPUExecutionProvider']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce99c107e1c483bbcec281fafeb252c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running ONNX Inference:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m session_options\u001b[38;5;241m.\u001b[39mgraph_optimization_level \u001b[38;5;241m=\u001b[39m ort\u001b[38;5;241m.\u001b[39mGraphOptimizationLevel\u001b[38;5;241m.\u001b[39mORT_ENABLE_ALL\n\u001b[1;32m      3\u001b[0m session \u001b[38;5;241m=\u001b[39m ort\u001b[38;5;241m.\u001b[39mInferenceSession(\n\u001b[1;32m      4\u001b[0m     ONNX_PATH, \n\u001b[1;32m      5\u001b[0m     sess_options\u001b[38;5;241m=\u001b[39msession_options, \n\u001b[1;32m      6\u001b[0m     providers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensorRTExecutionProvider\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43meval_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36meval_fn\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m      5\u001b[0m     input_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, IMG_SIZE, IMG_SIZE)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      6\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m----> 7\u001b[0m     __ \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     inference_times\u001b[38;5;241m.\u001b[39mappend((time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e3\u001b[39m) \u001b[38;5;66;03m# Convert to milliseconds\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:287\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    285\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "session_options = ort.SessionOptions()\n",
    "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "session = ort.InferenceSession(\n",
    "    ONNX_PATH, \n",
    "    sess_options=session_options, \n",
    "    providers = ['TensorRTExecutionProvider']\n",
    ")\n",
    "eval_fn(session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
